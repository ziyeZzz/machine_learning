{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(223)\n",
    "np.random.seed(223)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors and relation to Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch中tensor和numpy几乎一模一样\n",
    "\n",
    "但是numpy不会自动得到gradient，而torch的可以。两者也可以互相转换。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_numpy, x_torch\n",
      "[0.1 0.2 0.3] tensor([0.1000, 0.2000, 0.3000])\n"
     ]
    }
   ],
   "source": [
    "x_numpy = np.array([0.1, 0.2, 0.3])\n",
    "x_torch = torch.tensor([0.1, 0.2, 0.3])\n",
    "print('x_numpy, x_torch')\n",
    "print(x_numpy, x_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to and from numpy, pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1000, 0.2000, 0.3000], dtype=torch.float64) [0.1 0.2 0.3]\n"
     ]
    }
   ],
   "source": [
    "print(torch.from_numpy(x_numpy), x_torch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do basic operations like + - * /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x+y\n",
      "[3.1 4.2 5.3] tensor([3.1000, 4.2000, 5.3000])\n"
     ]
    }
   ],
   "source": [
    "y_numpy = np.array([3,4,5])\n",
    "y_torch = torch.tensor([3,4,5.])\n",
    "print(\"x+y\")\n",
    "print(x_numpy + y_numpy, x_torch+y_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most functions in numpy also in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm\n",
      "0.37416573867739417 tensor(0.3742)\n"
     ]
    }
   ],
   "source": [
    "print(\"norm\")\n",
    "print(np.linalg.norm(x_numpy), torch.norm(x_torch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply an operation along a dimension, we use the **dim** keyword argument instead of **axis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean along the 0th dimension\n",
      "[2. 3.] tensor([2., 3.])\n"
     ]
    }
   ],
   "source": [
    "print(\"mean along the 0th dimension\")\n",
    "x_numpy = np.array([[1,2],[3,4]])\n",
    "x_torch = torch.tensor([[1,2],[3,4.]])\n",
    "print(np.mean(x_numpy, axis=0), torch.mean(x_torch, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor.view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor.view() function reshape tensors, similarity to numpy.**reshape()**\n",
    "\n",
    "It can automatically calculate the correct dimension if a -1 is passed in. This is useful if we are working with batches but the batch size is unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 2, 28, 28])\n",
      "torch.Size([10000, 2, 784])\n",
      "torch.Size([10000, 2, 784])\n"
     ]
    }
   ],
   "source": [
    "# \"MNIST\"\n",
    "N, C, W, H = 10000, 2, 28, 28\n",
    "X = torch.randn(N, C, W, H)\n",
    "\n",
    "print(X.shape)\n",
    "print(X.view(N, C, 28*28).shape)\n",
    "print(X.view(-1, C, 28*28).shape) # automatically choose the 0the dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting semantics\n",
    "\n",
    "Two tensors are \"broadcastable\" if the following rules hold:\n",
    "\n",
    "+ Each tensor has at least one dimension\n",
    "+ When iterating over the dimension sizes, starting the tralling dimensions, the dimension sizes must either be equal, one of them is 1, one of them does not exit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[[[0.],\n",
      "          [0.],\n",
      "          [0.],\n",
      "          [0.]]],\n",
      "\n",
      "\n",
      "        [[[0.],\n",
      "          [0.],\n",
      "          [0.],\n",
      "          [0.]]],\n",
      "\n",
      "\n",
      "        [[[0.],\n",
      "          [0.],\n",
      "          [0.],\n",
      "          [0.]]],\n",
      "\n",
      "\n",
      "        [[[0.],\n",
      "          [0.],\n",
      "          [0.],\n",
      "          [0.]]],\n",
      "\n",
      "\n",
      "        [[[0.],\n",
      "          [0.],\n",
      "          [0.],\n",
      "          [0.]]]]) torch.Size([5, 1, 4, 1])\n",
      "y: tensor([[[-4.6264e-11]],\n",
      "\n",
      "        [[ 1.5449e-41]],\n",
      "\n",
      "        [[-4.6264e-11]]]) torch.Size([3, 1, 1])\n",
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "# pytorch operations support numpy broadcasting semantics\n",
    "# 两个不相同的向量，想直接加起来\n",
    "x = torch.empty(5, 1, 4, 1)\n",
    "y = torch.empty(3, 1, 1)\n",
    "print(\"x:\",x, x.shape)\n",
    "print(\"y:\",y, y.shape)\n",
    "print((x+y).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computation graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch tensor object implicitly creates a computation graph in the background.\n",
    "\n",
    "A computation graph is a way of writing mathematical expression as a graph.\n",
    "\n",
    "There is an algorithm to compute the gradients of all the variables of a computation graph in time on the same order it is to compute the function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c tensor(3., grad_fn=<AddBackward0>)\n",
      "d tensor(2., grad_fn=<AddBackward0>)\n",
      "e tensor(6., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# set requires_grad=True to let Pytorch know to keep the graph\n",
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "c = a+b\n",
    "d = b+1\n",
    "e = c*d\n",
    "print('c',c)\n",
    "print(\"d\",d)\n",
    "print('e',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUDA SEMANTICS\n",
    "\n",
    "It is easy to copy tensor between cpu and gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7387, 0.7572, 0.8219, 0.2490, 0.9072, 0.1679, 0.6588, 0.4689, 0.6731,\n",
      "        0.8119])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouzy/softwares/Python-3.7.3/install/lib/python3.7/site-packages/torch/cuda/__init__.py:87: UserWarning: \n",
      "    Found GPU1 Quadro K600 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7387, 0.7572, 0.8219, 0.2490, 0.9072, 0.1679, 0.6588, 0.4689, 0.6731,\n",
      "        0.8119], device='cuda:0')\n",
      "tensor([0.7387, 0.7572, 0.8219, 0.2490, 0.9072, 0.1679, 0.6588, 0.4689, 0.6731,\n",
      "        0.8119])\n"
     ]
    }
   ],
   "source": [
    "cpu = torch.device(\"cpu\")\n",
    "gpu = torch.device(\"cuda\")\n",
    "\n",
    "x = torch.rand(10)\n",
    "print(x)\n",
    "x = x.to(gpu)\n",
    "print(x)\n",
    "x = x.to(cpu)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch is an auto grad framework\n",
    "\n",
    "Consider the function $f(x) = (x-2)^2$\n",
    "\n",
    "Q: Compute $\\frac{d}{dx} f(x)$ and then compute $f'(1)$.\n",
    "\n",
    "**backward()** call on the leaf variable(y) in the computation, computes all the gradients of y at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical f'(x): tensor([-2.], grad_fn=<MulBackward0>)\n",
      "Pytorch back f'(x): tensor([-2.])\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return (x-2)**2\n",
    "\n",
    "def fp(x):\n",
    "    return 2*(x-2)\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "y = f(x)\n",
    "y.backward()\n",
    "\n",
    "print('Analytical f\\'(x):', fp(x))\n",
    "print('Pytorch back f\\'(x):', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have gradients, we can use our favorite optimization algorithm: gradient descent\n",
    "\n",
    "Let $f$ be the same function we defined above\n",
    "\n",
    "Q: Which x minimizes $f$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter, \tx, \tf(x), \tf'(x), \tf'(x) pytorch\n",
      "0, \t5.000, \t9.000, \t6.000, \t6.000\n",
      "1, \t3.500, \t2.250, \t3.000, \t3.000\n",
      "2, \t2.750, \t0.562, \t1.500, \t1.500\n",
      "3, \t2.375, \t0.141, \t0.750, \t0.750\n",
      "4, \t2.188, \t0.035, \t0.375, \t0.375\n",
      "5, \t2.094, \t0.009, \t0.188, \t0.188\n",
      "6, \t2.047, \t0.002, \t0.094, \t0.094\n",
      "7, \t2.023, \t0.001, \t0.047, \t0.047\n",
      "8, \t2.012, \t0.000, \t0.023, \t0.023\n",
      "9, \t2.006, \t0.000, \t0.012, \t0.012\n",
      "10, \t2.003, \t0.000, \t0.006, \t0.006\n",
      "11, \t2.001, \t0.000, \t0.003, \t0.003\n",
      "12, \t2.001, \t0.000, \t0.001, \t0.001\n",
      "13, \t2.000, \t0.000, \t0.001, \t0.001\n",
      "14, \t2.000, \t0.000, \t0.000, \t0.000\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.0], requires_grad=True)\n",
    "step_size = 0.25\n",
    "\n",
    "print('iter, \\tx, \\tf(x), \\tf\\'(x), \\tf\\'(x) pytorch')\n",
    "for i in range(15):\n",
    "    y = f(x)\n",
    "    y.backward() #compute the gradient\n",
    "    \n",
    "    print('{}, \\t{:.3f}, \\t{:.3f}, \\t{:.3f}, \\t{:.3f}'.format(i, x.item(), f(x).item(), fp(x).item(), x.grad.item()))\n",
    "    \n",
    "    # .data返回的是一个tensor\n",
    "    # 而.item()返回的是一个具体的数值\n",
    "    x.data = x.data -step_size*x.grad # perform a GD update step\n",
    "    \n",
    "    # we need to zero the grad variable since the backward() call\n",
    "    # accumulates the gradients in .grad instead of overwriting\n",
    "    # The detach_() is for efficiency.\n",
    "    # detach简单来说，就是截断反向传播的梯度流\n",
    "    x.grad.detach_()\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "Lets minimize a loss function on some made-up data\n",
    "\n",
    "We will implement Gradient Descent to solve linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 2]) torch.Size([50, 1]) torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# make a simple linear dataset with some noise\n",
    "\n",
    "d = 2\n",
    "n = 50\n",
    "X = torch.randn(n, d)\n",
    "true_w = torch.tensor([[-1.0], [2.0]])\n",
    "\n",
    "#在python 3.5以后，@是一个操作符，表示矩阵-向量乘法\n",
    "#A@x 就是矩阵-向量乘法A*x: np.dot(A, x)\n",
    "y = X@true_w + torch.randn(n, 1)*0.1\n",
    "\n",
    "print(X.shape, y.shape, true_w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient: [ 2.2574904 -3.780513 ]\n",
      "Pytorch gradient: [ 2.2574897 -3.7805133]\n"
     ]
    }
   ],
   "source": [
    "# define a linear model with no bias\n",
    "def model(X, w):\n",
    "    return X @ w\n",
    "\n",
    "# the residual sum of squares loss function\n",
    "def rss(y, y_hat):\n",
    "    return torch.norm(y-y_hat)**2/n\n",
    "\n",
    "# analytical expression for the gradient\n",
    "def grad_rss(X, y, w):\n",
    "    return -2*X.t()@(y - X@w)/n\n",
    "\n",
    "w = torch.tensor([[1.], [0]], requires_grad=True)\n",
    "y_hat = model(X, w)\n",
    "\n",
    "loss = rss(y, y_hat)\n",
    "loss.backward()\n",
    "\n",
    "print('Analytical gradient:', grad_rss(X, y, w).detach().view(2).numpy())\n",
    "print('Pytorch gradient:', w.grad.view(2).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using GD with automatically computes derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter, \tloss, \tw\n",
      "0, \t0.01, \t[-1.0010303  2.0016782]\n",
      "1, \t0.01, \t[-1.0010928  1.9975625]\n",
      "2, \t0.01, \t[-1.0012329  1.9943274]\n",
      "3, \t0.01, \t[-1.0014182  1.9917825]\n",
      "4, \t0.01, \t[-1.0016263  1.989779 ]\n",
      "5, \t0.01, \t[-1.0018413  1.9882004]\n",
      "6, \t0.01, \t[-1.0020531  1.9869554]\n",
      "7, \t0.01, \t[-1.0022551  1.9859726]\n",
      "8, \t0.01, \t[-1.0024433  1.9851962]\n",
      "9, \t0.01, \t[-1.0026157  1.9845821]\n",
      "10, \t0.01, \t[-1.0027715  1.9840958]\n",
      "11, \t0.01, \t[-1.0029109  1.9837104]\n",
      "12, \t0.01, \t[-1.0030344  1.9834046]\n",
      "13, \t0.01, \t[-1.0031432  1.9831617]\n",
      "14, \t0.01, \t[-1.0032384  1.9829684]\n",
      "15, \t0.01, \t[-1.0033213  1.9828147]\n",
      "16, \t0.01, \t[-1.0033932  1.982692 ]\n",
      "17, \t0.01, \t[-1.0034553  1.9825941]\n",
      "18, \t0.01, \t[-1.0035088  1.9825158]\n",
      "19, \t0.01, \t[-1.0035547  1.9824532]\n",
      "\n",
      "true w\t\t [-1.  2.]\n",
      "estimated w\t [-1.0035547  1.9824532]\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.1\n",
    "\n",
    "print('iter, \\tloss, \\tw')\n",
    "for i in range(20):\n",
    "    y_hat = model(X, w)\n",
    "    loss = rss(y, y_hat)\n",
    "    \n",
    "    loss.backward() # compute the gradient of the loss\n",
    "    \n",
    "    w.data = w.data - step_size * w.grad # do a gradient descent step\n",
    "    \n",
    "    print('{}, \\t{:.2f}, \\t{}'.format(i, loss.item(), w.view(2).detach().numpy()))\n",
    "    \n",
    "    # we need to zero the grad variable since the backward()\n",
    "    # call accumulates the gradients in .grad instead of overwriting.\n",
    "    w.grad.detach() #for efficiency, 这一句也可以去掉\n",
    "    w.grad.zero_()\n",
    "\n",
    "print('\\ntrue w\\t\\t', true_w.view(2).numpy())\n",
    "print('estimated w\\t', w.view(2).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.nn.Module\n",
    "\n",
    "Module is PyTorch's way of performing operations in tensors. \n",
    "\n",
    "Modules are implemented as subclasses of the torch.nn.Module class. All modules are callable and can be compossed together to create complex functions.\n",
    "\n",
    "torch.nn [docs](https://pytorch.org/docs/stable/nn.html)\n",
    "\n",
    "Note: most of the functionality implemented for modules can be accessed in a functional form via torch.nn.functional. But these require you to create and manage the weight tensors yourself.\n",
    "\n",
    "torch.nn.functional [docs](https://pytorch.org/docs/stable/nn.html#torch-nn-functional)\n",
    "\n",
    "### Linear Module\n",
    "\n",
    "It takes the input and output dimensions as parameters, and creates the weights in the object.\n",
    "\n",
    "Unlike we initialized our $w$ manually, the Linear module **automatically** initializes the weights **randomly**. \n",
    "\n",
    "For minimizing non convex loss function(e.g. neural networks), initialization is important and can affect results. If training is not working as expected, **try to manually initializing the weights** different from the default. Pytorch implements some common initializations in torch.nn.init.\n",
    "\n",
    "torch.nn.init [docs](https://pytorch.org/docs/stable/nn.html#torch-nn-init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: Parameter containing:\n",
      "tensor([[-0.3970,  0.1492, -0.4727],\n",
      "        [ 0.3516,  0.1003, -0.1688],\n",
      "        [ 0.3627,  0.2657, -0.0562],\n",
      "        [ 0.5693, -0.0235,  0.4763]], requires_grad=True) , b: Parameter containing:\n",
      "tensor([ 0.0619, -0.5457,  0.0056, -0.1865], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "d_in = 3\n",
    "d_out = 4\n",
    "linear_module = nn.Linear(d_in, d_out)\n",
    "\n",
    "example_tensor = torch.tensor([[1., 2, 3], [4, 5, 6]])\n",
    "# applys a linear transformation to the data\n",
    "transformed = linear_module(example_tensor)\n",
    "\n",
    "print('w:', linear_module.weight, ', b:', linear_module.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Pytorch implements a number of activation functions including ReLU, Tanh, Sigmoid.\n",
    "\n",
    "Since they are modules, they need to be instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example tensor: tensor([-1.,  1.,  0.])\n",
      "activated: tensor([0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "activation_fn = nn.ReLU() # we instantiate an instance of the ReLU module\n",
    "example_tensor = torch.tensor([-1.0, 1.0, 0.0])\n",
    "activated = activation_fn(example_tensor)\n",
    "print(\"example tensor:\",example_tensor)\n",
    "print(\"activated:\", activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential\n",
    "\n",
    "torch.nn.Sequential provides a good interface for composing simple modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed: tensor([[0.4031],\n",
      "        [0.5383]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "d_in = 3\n",
    "d_hidden = 4\n",
    "d_out = 1\n",
    "model = torch.nn.Sequential(\n",
    "                nn.Linear(d_in, d_hidden),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(d_hidden, d_out),\n",
    "                nn.Sigmoid()\n",
    "        )\n",
    "example_tensor = torch.tensor([[1., 2, 3], [4, 5, 6]])\n",
    "transformed = model(example_tensor)\n",
    "print(\"transformed:\", transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We can access all the parameters(of any nn.Module) with the $parameters()$ method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.4671,  0.2967, -0.2291],\n",
      "        [ 0.0093,  0.5333,  0.5189],\n",
      "        [-0.2274,  0.2020,  0.4496],\n",
      "        [-0.1831, -0.1396, -0.5206]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4898, -0.0881, -0.5568,  0.0661], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.4991, -0.4699,  0.1489, -0.2345]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2072], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "params = model.parameters()\n",
    "\n",
    "for param in params:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "\n",
    "PyTorch implements many common loss functions including $MSE Loss$ and $CrossEntropy Loss$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6667)\n"
     ]
    }
   ],
   "source": [
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "input = torch.tensor([[0., 0, 0]])\n",
    "target = torch.tensor([[1., 0, -1]])\n",
    "\n",
    "loss = mse_loss_fn(input, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.optim\n",
    "\n",
    "Pytorch implements a number of gradient-based optimization methods in $torch.optim$, including Gradient Descent. It takes in the model parameters and a learning rate at minimum.\n",
    "\n",
    "Optimizers do not compute the gradient for you. you must call **backward()** yourself. You also must call the **optim.zero_grad()** function **before calling backward()** since by default PyTorch does and inplace add the .grad member variable rather than overwriting it.\n",
    "\n",
    "zero_grad() does both the detach() and zero_() calls on tensor's grad variables.\n",
    "\n",
    "torch.optim [docs](https://pytorch.org/docs/stable/optim.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model params before: Parameter containing:\n",
      "tensor([[0.6078]], requires_grad=True)\n",
      "model params after: Parameter containing:\n",
      "tensor([[0.6347]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# create a simple model\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# create a simple dataset\n",
    "X_simple = torch.tensor([[1.]])\n",
    "y_simple = torch.tensor([[2.]])\n",
    "\n",
    "# create our optimizer\n",
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "y_hat = model(X_simple)\n",
    "print('model params before:', model.weight)\n",
    "loss = mse_loss_fn(y_hat, y_simple)\n",
    "optim.zero_grad()\n",
    "loss.backward()\n",
    "optim.step()\n",
    "\n",
    "print(\"model params after:\", model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression example\n",
    "\n",
    "#### Using GD with automatically computed derivatives and PyTorch's Modules\n",
    "\n",
    "Solve linear regression in a Pytorch way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter, \tloss, \tw\n",
      "0,\t6.94,\t[0.23284763 0.45748812]\n",
      "1,\t4.23,\t[-0.01339279  0.8107575 ]\n",
      "2,\t2.58,\t[-0.20953633  1.0830404 ]\n",
      "3,\t1.57,\t[-0.3659706  1.2927663]\n",
      "4,\t0.96,\t[-0.4908944  1.4541929]\n",
      "5,\t0.59,\t[-0.5907848  1.5783463]\n",
      "6,\t0.37,\t[-0.6707639  1.6737511]\n",
      "7,\t0.23,\t[-0.7348866  1.7469952]\n",
      "8,\t0.15,\t[-0.78636616  1.8031676 ]\n",
      "9,\t0.09,\t[-0.82775193  1.846198  ]\n",
      "10,\t0.06,\t[-0.8610687  1.8791192]\n",
      "11,\t0.04,\t[-0.88792664  1.9042706 ]\n",
      "12,\t0.03,\t[-0.9096076  1.923456 ]\n",
      "13,\t0.02,\t[-0.9271335  1.9380647]\n",
      "14,\t0.02,\t[-0.94131994  1.9491668 ]\n",
      "15,\t0.01,\t[-0.95281875  1.9575853 ]\n",
      "16,\t0.01,\t[-0.9621516  1.963953 ]\n",
      "17,\t0.01,\t[-0.9697364  1.968756 ]\n",
      "18,\t0.01,\t[-0.9759086  1.9723668]\n",
      "19,\t0.01,\t[-0.9809376  1.9750714]\n",
      "\n",
      "true w\t\t [-1.  2.]\n",
      "estimated w\t [-0.9809376  1.9750714]\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.1\n",
    "\n",
    "linear_module = nn.Linear(d, 1, bias=False)\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)\n",
    "\n",
    "print('iter, \\tloss, \\tw')\n",
    "\n",
    "for i in range(20):\n",
    "    y_hat = linear_module(X)\n",
    "    loss = loss_func(y_hat, y)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    print('{},\\t{:.2f},\\t{}'.format(i, loss.item(), linear_module.weight.view(2).detach().numpy()))\n",
    "\n",
    "print('\\ntrue w\\t\\t', true_w.view(2).numpy())\n",
    "print('estimated w\\t', linear_module.weight.view(2).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear regression with SGD\n",
    "\n",
    "previous examples, we computed the average gradient over the entire dataset(Gradient Descent). We can implement Stochastic Gradient Descent with a simple modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter, \tloss, \tw\n",
      "0,\t4.51,\t[-0.53437257  0.02807745]\n",
      "20,\t0.13,\t[-0.8789126  2.0364408]\n",
      "40,\t0.01,\t[-1.0130086  1.9781419]\n",
      "60,\t0.00,\t[-1.0464606  1.9671909]\n",
      "80,\t0.02,\t[-1.0403302  1.9768254]\n",
      "100,\t0.00,\t[-1.0423443  1.9765073]\n",
      "120,\t0.01,\t[-1.0502714  1.9967372]\n",
      "140,\t0.00,\t[-1.0339669  1.9758096]\n",
      "160,\t0.01,\t[-0.9581799  2.0230825]\n",
      "180,\t0.01,\t[-1.0368613  1.953854 ]\n",
      "\n",
      "true w\t\t [-1.  2.]\n",
      "estimated w\t [-0.9779671  1.9938502]\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.1\n",
    "\n",
    "linear_module = nn.Linear(d, 1, bias=False)\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "optim = torch.optim.SGD(linear_module.parameters(), lr=step_size)\n",
    "\n",
    "print('iter, \\tloss, \\tw')\n",
    "\n",
    "for i in range(200):\n",
    "    rand_idx = np.random.choice(n) # take a random point from the dataset\n",
    "    x = X[rand_idx]\n",
    "    y_hat = linear_module(x)\n",
    "    loss = loss_func(y_hat, y[rand_idx]) # only compute the loss on the single point\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print('{},\\t{:.2f},\\t{}'.format(i, loss.item(), linear_module.weight.view(2).detach().numpy()))\n",
    "\n",
    "print('\\ntrue w\\t\\t', true_w.view(2).numpy())\n",
    "print('estimated w\\t', linear_module.weight.view(2).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Basics in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "d = 1 \n",
    "n = 200\n",
    "X = torch.rand(n, d)\n",
    "y = 4*torch.sin(np.pi * X) * torch.cos( 6 * np.pi * X**2)\n",
    "\n",
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.title('plot of $f(x)$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a simple two hidden layer neural network with Tanh activations. \n",
    "\n",
    "There are a few hyper parameter to play with to get a feel of how they change the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter,\tloss\n",
      "0, \t4.12\n",
      "600, \t3.46\n",
      "1200, \t1.46\n",
      "1800, \t0.67\n",
      "2400, \t0.49\n",
      "3000, \t0.18\n",
      "3600, \t0.13\n",
      "4200, \t0.23\n",
      "4800, \t0.10\n",
      "5400, \t0.03\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.1\n",
    "n_epochs = 6000\n",
    "n_hidden_1 = 32\n",
    "n_hidden_2 = 64\n",
    "d_out = 1\n",
    "\n",
    "neural_network = nn.Sequential(\n",
    "                    nn.Linear(d, n_hidden_1),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(n_hidden_1, n_hidden_2),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(n_hidden_2, d_out)\n",
    "                    )\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "optim = torch.optim.SGD(neural_network.parameters(), lr=step_size)\n",
    "print('iter,\\tloss')\n",
    "for i in range(n_epochs):\n",
    "    y_hat = neural_network(X)\n",
    "    loss = loss_func(y_hat, y)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if i % (n_epochs // 10) == 0:\n",
    "        print('{}, \\t{:.2f}'.format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_grid = torch.from_numpy(np.linspace(0, 1, 50)).float().view(-1, d)\n",
    "y_hat = neural_network(X_grid)\n",
    "plt.scatter(X.numpy(), y.numpy())\n",
    "plt.plot(X_grid.detach().numpy(), y_hat.detach().numpy(), 'r')\n",
    "\n",
    "plt.title('plot of $f(x)$ and $\\hat{f}(x)$')\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things helpful for homework\n",
    "\n",
    "### Brief Sidenote: Momentum\n",
    "\n",
    "There are other optimization algorithms besides SGD. One is a modification of SGD called momemtum. [ref](https://distill.pub/2017/momentum/)\n",
    "\n",
    "We change the step size and add the momentum keyword argument to the optimizer. Notice it reduces the training loss in fewer iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter, \tloss\n",
      "0,\t4.21\n",
      "150,\t1.13\n",
      "300,\t0.25\n",
      "450,\t0.06\n",
      "600,\t0.07\n",
      "750,\t0.02\n",
      "900,\t0.00\n",
      "1050,\t0.01\n",
      "1200,\t0.00\n",
      "1350,\t0.00\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.05\n",
    "momentum = 0.9\n",
    "n_epochs = 1500\n",
    "n_hidden_1 = 64\n",
    "n_hidden_2 = 32\n",
    "d_out = 1\n",
    "\n",
    "neural_network = nn.Sequential(\n",
    "                    nn.Linear(d, n_hidden_1),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(n_hidden_1, n_hidden_2),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Linear(n_hidden_2, d_out)\n",
    "                    )\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "optim = torch.optim.SGD(neural_network.parameters(), lr=step_size, momentum = momentum)\n",
    "\n",
    "print('iter, \\tloss')\n",
    "for i in range(n_epochs):\n",
    "    y_hat = neural_network(X)\n",
    "    loss = loss_func(y_hat, y)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if i % (n_epochs // 10) == 0:\n",
    "        print('{},\\t{:.2f}'.format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossEntropyLoss\n",
    "\n",
    "So far, we have been considering regression tasks and use MSELoss module. For homework, we will perform a classification task and will use cross entropy loss.\n",
    "\n",
    "The use of cross entropy loss in PyTorch is slightly different than MSE.\n",
    "+ input: The first parameter to CrossEntropyLoss is the output of our network. It expects a real valued tensor of dimensions(N, C) where **N is the minibatch size and C is the number of classes**. In our case, N=3 and C=2. The value along the second dimension correspond to raw unnormalized scores for each class. The crossEntropyLoss module does the softmax calculation for us, we do not need to apply our own softmax to the output.\n",
    "\n",
    "\n",
    "+ output: The second parameter is the true label. It expects an integer valued tensor of dimension (N). The interger at each element corresponds to the correct class. In our case, the \"correct\" class labels are class0, class1, class1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output1,\toutput2,\toutput3,\toutput4\n",
      "0.13,\t0.00,\t2.13,\t6.00\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "input1 = torch.tensor([[-1., 1], [-1, 1], [1, -1]])#相当于[1, 1, 0], loss小\n",
    "input2 = torch.tensor([[-3., 3], [-3, 3], [3, -3]])#相当于[1, 1, 0]，loss更小\n",
    "input3 = torch.tensor([[1., -1], [1, -1], [-1, 1]])#[0, 0, 1]，loss大\n",
    "input4 = torch.tensor([[3., -3], [3, -3], [-3, 3]])#[0, 0, 1]，loss更大\n",
    "\n",
    "target = torch.tensor([1, 1, 0])\n",
    "\n",
    "output1 = loss(input1, target)\n",
    "output2 = loss(input2, target)\n",
    "output3 = loss(input3, target)\n",
    "output4 = loss(input4, target)\n",
    "\n",
    "print('output1,\\toutput2,\\toutput3,\\toutput4')\n",
    "print('{:.2f},\\t{:.2f},\\t{:.2f},\\t{:.2f}'.format(output1, output2, output3, output4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate scheduler\n",
    "\n",
    "Since we do not want to use a fixed learning rate thoughout all training. PyTorch offers learning rate schedulers to change the learning rate over time. \n",
    "\n",
    "Common strategies include multiplying the lr by a constant every epoch(e.g. 0.9) and halving the learning rate when the training rate flattens out.\n",
    "\n",
    "See the [learning rate scheduler docs](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate) for usage and examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutions\n",
    "\n",
    "When working with images, we usually use conv to extract features. PyTorch implements this for us in the $torch.nn.Conv2d$ module. \n",
    "\n",
    "It expects the input to have a specific dimension $(N, C_{in}, H_{in}, W_{in})$ where $N$ is batch size, $C_{in}$ is the number of channels the image has, and $H_{in}, W_{in}$ are the image height and width respectively.\n",
    "\n",
    "We can modify following parameters:\n",
    "\n",
    "+ kernel_size\n",
    "+ stride\n",
    "+ padding\n",
    "\n",
    "They can change the output dimension, so be careful\n",
    "\n",
    "See the [torch.nn.Conv2d docs](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d) for more information.\n",
    "\n",
    "To illustrate what $Conv2d$ module is doing, let us set the weights manually to a Gaussian blur kernel.\n",
    "\n",
    "We can see it applies the kernel to the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an entire mnist digit\n",
    "image = np.array([0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.3803922 , 0.37647063, 0.3019608 ,0.46274513, 0.2392157 , 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.3529412 , 0.5411765 , 0.9215687 ,0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 , 0.9215687 ,0.9843138 , 0.9843138 , 0.9725491 , 0.9960785 , 0.9607844 ,0.9215687 , 0.74509805, 0.08235294, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.54901963,0.9843138 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.7411765 , 0.09019608, 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.8862746 , 0.9960785 , 0.81568635,0.7803922 , 0.7803922 , 0.7803922 , 0.7803922 , 0.54509807,0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 , 0.2392157 ,0.5019608 , 0.8705883 , 0.9960785 , 0.9960785 , 0.7411765 ,0.08235294, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.14901961, 0.32156864, 0.0509804 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.13333334,0.8352942 , 0.9960785 , 0.9960785 , 0.45098042, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.32941177, 0.9960785 ,0.9960785 , 0.9176471 , 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.32941177, 0.9960785 , 0.9960785 , 0.9176471 ,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.4156863 , 0.6156863 ,0.9960785 , 0.9960785 , 0.95294124, 0.20000002, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.09803922, 0.45882356, 0.8941177 , 0.8941177 ,0.8941177 , 0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.94117653, 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.26666668, 0.4666667 , 0.86274517,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 ,0.9960785 , 0.9960785 , 0.9960785 , 0.9960785 , 0.5568628 ,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.14509805, 0.73333335,0.9921569 , 0.9960785 , 0.9960785 , 0.9960785 , 0.8745099 ,0.8078432 , 0.8078432 , 0.29411766, 0.26666668, 0.8431373 ,0.9960785 , 0.9960785 , 0.45882356, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.4431373 , 0.8588236 , 0.9960785 , 0.9490197 , 0.89019614,0.45098042, 0.34901962, 0.12156864, 0., 0.,0., 0., 0.7843138 , 0.9960785 , 0.9450981 ,0.16078432, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.6627451 , 0.9960785 ,0.6901961 , 0.24313727, 0., 0., 0.,0., 0., 0., 0., 0.18823531,0.9058824 , 0.9960785 , 0.9176471 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0.07058824, 0.48627454, 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.32941177, 0.9960785 , 0.9960785 ,0.6509804 , 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.54509807, 0.9960785 , 0.9333334 , 0.22352943, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.8235295 , 0.9803922 , 0.9960785 ,0.65882355, 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0.9490197 , 0.9960785 , 0.93725497, 0.22352943, 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.34901962, 0.9843138 , 0.9450981 ,0.3372549 , 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.01960784,0.8078432 , 0.96470594, 0.6156863 , 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0.01568628, 0.45882356, 0.27058825,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0., 0.,0., 0., 0., 0.], dtype=np.float32)\n",
    "image_torch = torch.from_numpy(image).view(1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b11e9f21cc0>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a gaussian blur kernel\n",
    "gaussian_kernel = torch.tensor([[1., 2, 1], [2, 4, 2], [1, 2, 1]]) / 16.0\n",
    "\n",
    "conv = nn.Conv2d(1, 1, 3)\n",
    "# manually set the conv weight\n",
    "conv.weight.data[:] = gaussian_kernel\n",
    "\n",
    "convolved = conv(image_torch)\n",
    "\n",
    "plt.title('org image')\n",
    "plt.imshow(image_torch.view(28, 28).detach().numpy())\n",
    "plt.show()\n",
    "\n",
    "plt.title('blurred image')\n",
    "plt.imshow(convolved.view(26, 26).detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image is blurred as expected.\n",
    "\n",
    "In practice, we learn many kernels at a time. In this example, we take in an RGB image(3 channels) and output a 16 channels image. \n",
    "\n",
    "After an activation function, that could be used as input to another Conv2d module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im shape: torch.Size([4, 3, 32, 32])\n",
      "convolved shape: torch.Size([4, 16, 30, 30])\n"
     ]
    }
   ],
   "source": [
    "im_channels = 3 # RGB images are 3 channels; Black and White, 1 channel\n",
    "out_channels = 16 # hyperparameter we can tune\n",
    "kernel_size = 3 # hyperparameter we can tune\n",
    "batch_size = 4\n",
    "image_width = 32\n",
    "image_height = 32\n",
    "\n",
    "im = torch.randn(batch_size, im_channels, image_width, image_height)\n",
    "\n",
    "conv = nn.Conv2d(im_channels, out_channels, kernel_size)\n",
    "convolved = conv(im) # a module we can call\n",
    "\n",
    "print(\"im shape:\", im.shape)\n",
    "print('convolved shape:', convolved.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful links:\n",
    "\n",
    "+ [60 minute PyTorch Tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "+ [PyTorch Docs](https://pytorch.org/docs/stable/index.html)\n",
    "+ [Lecture notes on Auto-Diff](https://courses.cs.washington.edu/courses/cse446/19wi/notes/auto-diff.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Datasets, DataLoaders\n",
    "\n",
    "This is modified from pytorch official tutorial.<https://chsasank.github.io>\n",
    "\n",
    "PyTorch provides many tools to prepare data, make data loading easy and code more readable.\n",
    "\n",
    "In this tutorial, we will see how to load and preprocess/augment data from a non trivial dataset.\n",
    "\n",
    "### Dataset class\n",
    "\n",
    "$torch.utils.data.Dataset$ is an abstract class representing a dataset. Your custom dataset should inherit $Dataset$ and override the follosing methods:\n",
    "\n",
    "+ __len__ so that len(dataset) returns the size of the dataset.\n",
    "+ __getitem__ to support the indexing such that dataset[i] can be used to get $i$ th sample\n",
    "\n",
    "Let us create a dataset class for our face landmarks dataset. We will read the csv in __int__ but leave the reading of images to __getitem__. This is memory efficient because all the images are not stored in the memory at once but read as required.\n",
    "\n",
    "Sample of our dataset will be a dict {'image': image, 'landmarks':landmark}. Our dataset will take an optional argument $transform$ so that any required processing can be applied on the sample.We will see the usefulness of $transform$ in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FakeDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a simple $for$ loop to iterate over the data, we are losing a lot of features. In particular:\n",
    "\n",
    "+ Batching the data\n",
    "+ Shuffling the data\n",
    "+ Load the data in parallel using $multiprocessing$ workers.\n",
    "\n",
    "$torch.utils.data.DataLoader$ is an iterator which provides all these features. Parameters used below should be clear. You can specify how exactly the samples need to be batched using $collate_fn$. However, default collate should work fine for most use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [tensor([[0.5731, 0.3522, 0.8924, 0.7387, 0.1603, 0.6738, 0.8475, 0.5324, 0.2867,\n",
      "         0.9676],\n",
      "        [0.6065, 0.7564, 0.6226, 0.1509, 0.7928, 0.6722, 0.2665, 0.0260, 0.8709,\n",
      "         0.4901],\n",
      "        [0.1946, 0.8254, 0.7107, 0.9242, 0.9325, 0.9566, 0.4936, 0.2932, 0.3205,\n",
      "         0.7291],\n",
      "        [0.9846, 0.5993, 0.0100, 0.3986, 0.8331, 0.0473, 0.9524, 0.0777, 0.5083,\n",
      "         0.3032]], dtype=torch.float64), tensor([0.3798, 0.0600, 0.2585, 0.2189], dtype=torch.float64)]\n",
      "1 [tensor([[0.7790, 0.8279, 0.0550, 0.0412, 0.0761, 0.8209, 0.3106, 0.3042, 0.8627,\n",
      "         0.7871],\n",
      "        [0.0785, 0.4358, 0.4960, 0.4975, 0.2461, 0.2620, 0.9665, 0.4101, 0.4390,\n",
      "         0.7586],\n",
      "        [0.8214, 0.3621, 0.4067, 0.4915, 0.5756, 0.0225, 0.3004, 0.0227, 0.6835,\n",
      "         0.3854],\n",
      "        [0.7188, 0.4558, 0.4876, 0.2920, 0.0415, 0.0202, 0.7813, 0.3701, 0.5999,\n",
      "         0.9614]], dtype=torch.float64), tensor([0.1186, 0.6269, 0.0943, 0.3152], dtype=torch.float64)]\n",
      "2 [tensor([[0.4776, 0.9418, 0.3406, 0.6062, 0.8557, 0.0091, 0.2603, 0.3207, 0.1236,\n",
      "         0.5581],\n",
      "        [0.1282, 0.9595, 0.8940, 0.3560, 0.7056, 0.8508, 0.8535, 0.4372, 0.7371,\n",
      "         0.3562],\n",
      "        [0.4791, 0.2160, 0.9924, 0.3058, 0.8337, 0.8880, 0.8871, 0.1436, 0.6167,\n",
      "         0.2302],\n",
      "        [0.5746, 0.2341, 0.7344, 0.9516, 0.8098, 0.2075, 0.6381, 0.5705, 0.9234,\n",
      "         0.0187]], dtype=torch.float64), tensor([0.9722, 0.1938, 0.4050, 0.1436], dtype=torch.float64)]\n",
      "3 [tensor([[0.5819, 0.9633, 0.2687, 0.0756, 0.1296, 0.5981, 0.4670, 0.1660, 0.7261,\n",
      "         0.9806],\n",
      "        [0.6949, 0.5806, 0.4443, 0.6947, 0.8026, 0.2632, 0.5656, 0.5717, 0.5182,\n",
      "         0.8633],\n",
      "        [0.2069, 0.9956, 0.2153, 0.8661, 0.7748, 0.3819, 0.1035, 0.6092, 0.5847,\n",
      "         0.2279],\n",
      "        [0.7958, 0.4718, 0.7230, 0.5673, 0.7555, 0.2388, 0.8735, 0.2222, 0.0375,\n",
      "         0.9220]], dtype=torch.float64), tensor([0.4618, 0.8794, 0.6587, 0.9317], dtype=torch.float64)]\n",
      "4 [tensor([[8.1999e-01, 9.4607e-01, 5.5716e-01, 1.3367e-01, 8.5730e-01, 9.9241e-01,\n",
      "         9.2539e-01, 6.3478e-01, 7.4073e-01, 3.0078e-01],\n",
      "        [7.5595e-01, 2.0748e-01, 6.4929e-01, 8.7415e-01, 7.4497e-01, 9.2496e-01,\n",
      "         5.5069e-01, 2.2143e-01, 9.8611e-01, 6.7910e-02],\n",
      "        [6.1169e-01, 2.3540e-01, 7.4069e-01, 4.8593e-01, 5.4977e-01, 8.8213e-01,\n",
      "         9.1613e-01, 5.2119e-01, 5.5956e-01, 5.4923e-01],\n",
      "        [5.1667e-04, 8.4435e-01, 3.3225e-01, 2.8762e-02, 9.4581e-01, 7.2648e-01,\n",
      "         2.8333e-01, 3.3605e-01, 2.6922e-01, 2.2783e-01]], dtype=torch.float64), tensor([0.2430, 0.1705, 0.4927, 0.6980], dtype=torch.float64)]\n",
      "5 [tensor([[0.4355, 0.4913, 0.7458, 0.7067, 0.2010, 0.0995, 0.4595, 0.0581, 0.4899,\n",
      "         0.9324],\n",
      "        [0.1065, 0.7949, 0.4263, 0.7580, 0.9607, 0.5227, 0.5349, 0.9763, 0.5128,\n",
      "         0.8355],\n",
      "        [0.4090, 0.7944, 0.7259, 0.7599, 0.1013, 0.4855, 0.7503, 0.3782, 0.0036,\n",
      "         0.0236],\n",
      "        [0.0698, 0.3932, 0.3745, 0.7247, 0.2771, 0.0285, 0.2752, 0.9096, 0.1587,\n",
      "         0.3099]], dtype=torch.float64), tensor([0.0594, 0.9745, 0.1807, 0.8092], dtype=torch.float64)]\n",
      "6 [tensor([[0.5143, 0.7813, 0.3657, 0.3446, 0.4107, 0.0594, 0.9986, 0.6698, 0.5079,\n",
      "         0.6787],\n",
      "        [0.3667, 0.6060, 0.6104, 0.7239, 0.3005, 0.4896, 0.3851, 0.6697, 0.1840,\n",
      "         0.7050],\n",
      "        [0.6251, 0.6534, 0.7699, 0.0052, 0.3902, 0.1319, 0.7581, 0.1246, 0.8619,\n",
      "         0.0764],\n",
      "        [0.8679, 0.9947, 0.9261, 0.4871, 0.7949, 0.5663, 0.4499, 0.7330, 0.6093,\n",
      "         0.6378]], dtype=torch.float64), tensor([0.3140, 0.2377, 0.2452, 0.1152], dtype=torch.float64)]\n",
      "7 [tensor([[0.7900, 0.7897, 0.1925, 0.8234, 0.2290, 0.3376, 0.6540, 0.7616, 0.2335,\n",
      "         0.2385],\n",
      "        [0.5711, 0.8307, 0.2810, 0.1290, 0.9470, 0.9564, 0.8077, 0.5323, 0.5188,\n",
      "         0.4702],\n",
      "        [0.8652, 0.6578, 0.4784, 0.5538, 0.8735, 0.9332, 0.5884, 0.6665, 0.1644,\n",
      "         0.1446],\n",
      "        [0.3033, 0.0011, 0.4145, 0.4263, 0.7541, 0.9564, 0.1828, 0.4769, 0.4277,\n",
      "         0.2744]], dtype=torch.float64), tensor([0.4914, 0.3992, 0.9444, 0.9147], dtype=torch.float64)]\n",
      "8 [tensor([[0.1745, 0.2732, 0.5760, 0.9013, 0.8982, 0.0243, 0.9831, 0.1041, 0.8782,\n",
      "         0.6570],\n",
      "        [0.2827, 0.2042, 0.6947, 0.8491, 0.9601, 0.3865, 0.4846, 0.7964, 0.3080,\n",
      "         0.6688],\n",
      "        [0.7616, 0.2483, 0.1599, 0.5594, 0.7716, 0.0644, 0.2717, 0.4100, 0.3545,\n",
      "         0.0580],\n",
      "        [0.0084, 0.3714, 0.5732, 0.8936, 0.0660, 0.0622, 0.4123, 0.3981, 0.1786,\n",
      "         0.1452]], dtype=torch.float64), tensor([0.7544, 0.1739, 0.6046, 0.1710], dtype=torch.float64)]\n",
      "9 [tensor([[0.7719, 0.8116, 0.6556, 0.7419, 0.7317, 0.2478, 0.0722, 0.1898, 0.6427,\n",
      "         0.8502],\n",
      "        [0.0178, 0.8890, 0.5330, 0.1007, 0.1229, 0.6800, 0.2524, 0.9572, 0.0589,\n",
      "         0.7010],\n",
      "        [0.9366, 0.5576, 0.6789, 0.3838, 0.6987, 0.3774, 0.9817, 0.5304, 0.4717,\n",
      "         0.8181],\n",
      "        [0.6829, 0.7412, 0.6690, 0.0556, 0.3994, 0.2422, 0.4519, 0.6155, 0.5241,\n",
      "         0.1712]], dtype=torch.float64), tensor([0.4873, 0.0892, 0.0198, 0.4107], dtype=torch.float64)]\n",
      "10 [tensor([[0.5134, 0.0761, 0.0098, 0.6416, 0.3197, 0.6796, 0.1917, 0.4490, 0.4056,\n",
      "         0.9849],\n",
      "        [0.5045, 0.6041, 0.0491, 0.4489, 0.1539, 0.3086, 0.7788, 0.4410, 0.6982,\n",
      "         0.2020],\n",
      "        [0.2837, 0.5344, 0.8749, 0.5130, 0.6066, 0.5833, 0.5600, 0.2467, 0.3212,\n",
      "         0.5832],\n",
      "        [0.0571, 0.5563, 0.9788, 0.3461, 0.0305, 0.4120, 0.4674, 0.2641, 0.5315,\n",
      "         0.5246]], dtype=torch.float64), tensor([0.6941, 0.2978, 0.3688, 0.6870], dtype=torch.float64)]\n",
      "11 [tensor([[0.9368, 0.3781, 0.1898, 0.1834, 0.3785, 0.0990, 0.1178, 0.2504, 0.3109,\n",
      "         0.3586],\n",
      "        [0.6367, 0.3671, 0.8628, 0.7081, 0.5434, 0.0434, 0.7340, 0.9068, 0.0232,\n",
      "         0.7988],\n",
      "        [0.1926, 0.4436, 0.3273, 0.7181, 0.7908, 0.6022, 0.3342, 0.0906, 0.2368,\n",
      "         0.2496],\n",
      "        [0.2944, 0.5184, 0.4559, 0.0047, 0.9323, 0.4420, 0.7640, 0.6010, 0.5341,\n",
      "         0.5950]], dtype=torch.float64), tensor([0.2982, 0.0284, 0.1100, 0.8063], dtype=torch.float64)]\n",
      "12 [tensor([[0.3546, 0.5512, 0.3533, 0.4910, 0.0021, 0.0281, 0.3072, 0.3737, 0.4022,\n",
      "         0.0214],\n",
      "        [0.7541, 0.2178, 0.9857, 0.6581, 0.8062, 0.4953, 0.1533, 0.6880, 0.2804,\n",
      "         0.6484],\n",
      "        [0.1922, 0.5623, 0.3538, 0.9590, 0.8139, 0.5680, 0.7111, 0.0994, 0.5041,\n",
      "         0.1535],\n",
      "        [0.9090, 0.4082, 0.8595, 0.6328, 0.3985, 0.1027, 0.2872, 0.2958, 0.4326,\n",
      "         0.8404]], dtype=torch.float64), tensor([0.3132, 0.8479, 0.0641, 0.1908], dtype=torch.float64)]\n",
      "13 [tensor([[0.1235, 0.5906, 0.2151, 0.1735, 0.9581, 0.3833, 0.3754, 0.2902, 0.4526,\n",
      "         0.8877],\n",
      "        [0.0746, 0.6009, 0.3356, 0.1052, 0.8998, 0.9239, 0.9592, 0.0888, 0.4581,\n",
      "         0.6264],\n",
      "        [0.7043, 0.1803, 0.9710, 0.2595, 0.4442, 0.5638, 0.7992, 0.7294, 0.1056,\n",
      "         0.0821],\n",
      "        [0.2075, 0.0542, 0.2082, 0.7640, 0.2075, 0.7804, 0.8667, 0.7703, 0.4880,\n",
      "         0.4175]], dtype=torch.float64), tensor([0.8044, 0.5209, 0.1167, 0.1043], dtype=torch.float64)]\n",
      "14 [tensor([[0.7110, 0.0324, 0.6999, 0.3120, 0.3443, 0.1947, 0.1145, 0.1388, 0.3361,\n",
      "         0.2436],\n",
      "        [0.2712, 0.8417, 0.3924, 0.1672, 0.1324, 0.2891, 0.4626, 0.4259, 0.5827,\n",
      "         0.1295],\n",
      "        [0.7321, 0.5713, 0.9735, 0.9981, 0.3251, 0.4321, 0.6025, 0.7267, 0.9462,\n",
      "         0.5622],\n",
      "        [0.3791, 0.2555, 0.3161, 0.3799, 0.5379, 0.3367, 0.0179, 0.1942, 0.8255,\n",
      "         0.2560]], dtype=torch.float64), tensor([0.0944, 0.9346, 0.8076, 0.7030], dtype=torch.float64)]\n",
      "15 [tensor([[0.2112, 0.0674, 0.2896, 0.8931, 0.0379, 0.9509, 0.8315, 0.6334, 0.7816,\n",
      "         0.1188],\n",
      "        [0.0221, 0.3269, 0.6086, 0.2643, 0.9615, 0.9147, 0.8151, 0.4617, 0.8872,\n",
      "         0.7171],\n",
      "        [0.1492, 0.7751, 0.5907, 0.4136, 0.6763, 0.8151, 0.3424, 0.9665, 0.9925,\n",
      "         0.0646],\n",
      "        [0.5238, 0.3191, 0.7562, 0.5360, 0.8993, 0.0396, 0.2865, 0.2601, 0.3873,\n",
      "         0.6201]], dtype=torch.float64), tensor([0.1608, 0.4188, 0.0682, 0.8259], dtype=torch.float64)]\n",
      "16 [tensor([[0.4315, 0.9746, 0.9354, 0.7737, 0.3274, 0.7253, 0.2168, 0.6019, 0.6914,\n",
      "         0.5053],\n",
      "        [0.5331, 0.3018, 0.5681, 0.5742, 0.4409, 0.8460, 0.5683, 0.1129, 0.8864,\n",
      "         0.3009],\n",
      "        [0.3475, 0.2054, 0.3291, 0.6695, 0.8217, 0.9327, 0.9285, 0.7978, 0.1082,\n",
      "         0.9808],\n",
      "        [0.7949, 0.7450, 0.1057, 0.9749, 0.3307, 0.2010, 0.0066, 0.2680, 0.1572,\n",
      "         0.7535]], dtype=torch.float64), tensor([0.2018, 0.7790, 0.2934, 0.4572], dtype=torch.float64)]\n",
      "17 [tensor([[0.1234, 0.2763, 0.6250, 0.5892, 0.1478, 0.2381, 0.7827, 0.4213, 0.8896,\n",
      "         0.3556],\n",
      "        [0.8804, 0.3150, 0.0932, 0.8300, 0.2015, 0.1168, 0.4564, 0.0089, 0.4918,\n",
      "         0.1978],\n",
      "        [0.5509, 0.3448, 0.5404, 0.9125, 0.3244, 0.3438, 0.1229, 0.6017, 0.6842,\n",
      "         0.6517],\n",
      "        [0.6252, 0.8326, 0.7335, 0.5683, 0.4805, 0.0205, 0.8423, 0.8335, 0.0611,\n",
      "         0.5420]], dtype=torch.float64), tensor([0.7734, 0.9829, 0.4052, 0.8478], dtype=torch.float64)]\n",
      "18 [tensor([[0.8319, 0.0190, 0.6834, 0.4361, 0.5816, 0.8063, 0.6133, 0.8275, 0.5506,\n",
      "         0.4941],\n",
      "        [0.9471, 0.6152, 0.7326, 0.3613, 0.0433, 0.4246, 0.7528, 0.9425, 0.9434,\n",
      "         0.0835],\n",
      "        [0.0286, 0.4711, 0.6294, 0.1529, 0.7598, 0.7471, 0.1501, 0.7979, 0.2794,\n",
      "         0.0701],\n",
      "        [0.9865, 0.6917, 0.4031, 0.6475, 0.4017, 0.7013, 0.3768, 0.5441, 0.6133,\n",
      "         0.9827]], dtype=torch.float64), tensor([0.2432, 0.4096, 0.4214, 0.3625], dtype=torch.float64)]\n",
      "19 [tensor([[0.1595, 0.1620, 0.6766, 0.7347, 0.6800, 0.1595, 0.6351, 0.9497, 0.2426,\n",
      "         0.6980],\n",
      "        [0.8173, 0.7557, 0.0332, 0.8683, 0.8946, 0.8717, 0.1310, 0.7948, 0.3232,\n",
      "         0.3358],\n",
      "        [0.7858, 0.6377, 0.2486, 0.6115, 0.0269, 0.4160, 0.4587, 0.8693, 0.7665,\n",
      "         0.7337],\n",
      "        [0.2460, 0.2389, 0.0526, 0.4953, 0.1022, 0.3491, 0.8909, 0.7873, 0.9476,\n",
      "         0.1190]], dtype=torch.float64), tensor([0.7695, 0.1763, 0.3614, 0.8793], dtype=torch.float64)]\n",
      "20 [tensor([[0.9447, 0.4553, 0.3090, 0.9314, 0.5810, 0.7478, 0.1801, 0.7944, 0.3395,\n",
      "         0.7796],\n",
      "        [0.3616, 0.6536, 0.3887, 0.5575, 0.5016, 0.3341, 0.3248, 0.3028, 0.0241,\n",
      "         0.2205],\n",
      "        [0.1569, 0.6746, 0.1610, 0.6416, 0.5379, 0.0899, 0.8109, 0.1574, 0.6636,\n",
      "         0.1208],\n",
      "        [0.6763, 0.3114, 0.1191, 0.5024, 0.8485, 0.5508, 0.9937, 0.4441, 0.6268,\n",
      "         0.0124]], dtype=torch.float64), tensor([0.8330, 0.4588, 0.6610, 0.4125], dtype=torch.float64)]\n",
      "21 [tensor([[0.1652, 0.8297, 0.7624, 0.2486, 0.0530, 0.2290, 0.2390, 0.8085, 0.9943,\n",
      "         0.7612],\n",
      "        [0.3446, 0.4889, 0.6931, 0.5729, 0.6845, 0.1469, 0.3541, 0.1273, 0.8349,\n",
      "         0.5690],\n",
      "        [0.2602, 0.4863, 0.4534, 0.2980, 0.7037, 0.4775, 0.0909, 0.8940, 0.3209,\n",
      "         0.5321],\n",
      "        [0.0541, 0.3995, 0.1713, 0.3571, 0.8195, 0.9805, 0.3720, 0.2270, 0.1000,\n",
      "         0.0734]], dtype=torch.float64), tensor([0.2157, 0.5211, 0.3512, 0.4156], dtype=torch.float64)]\n",
      "22 [tensor([[0.7748, 0.7320, 0.4631, 0.7669, 0.0317, 0.5785, 0.9670, 0.9411, 0.5266,\n",
      "         0.7897],\n",
      "        [0.6226, 0.6133, 0.9546, 0.8534, 0.2655, 0.5631, 0.2538, 0.1017, 0.7896,\n",
      "         0.7864],\n",
      "        [0.1106, 0.0427, 0.2253, 0.5984, 0.1476, 0.9428, 0.8574, 0.4992, 0.1863,\n",
      "         0.0076],\n",
      "        [0.6480, 0.0496, 0.1820, 0.0035, 0.7532, 0.9226, 0.3586, 0.1746, 0.0929,\n",
      "         0.3100]], dtype=torch.float64), tensor([0.0130, 0.5404, 0.7583, 0.5099], dtype=torch.float64)]\n",
      "23 [tensor([[0.0109, 0.2624, 0.4719, 0.2345, 0.1493, 0.9487, 0.3868, 0.3880, 0.1207,\n",
      "         0.9011],\n",
      "        [0.1786, 0.5431, 0.3357, 0.3656, 0.4052, 0.6589, 0.4147, 0.4342, 0.2196,\n",
      "         0.7168],\n",
      "        [0.5714, 0.6594, 0.8891, 0.5636, 0.0517, 0.0158, 0.0049, 0.4637, 0.5697,\n",
      "         0.5704],\n",
      "        [0.9934, 0.8142, 0.1091, 0.9558, 0.1230, 0.5885, 0.5156, 0.7678, 0.0012,\n",
      "         0.7585]], dtype=torch.float64), tensor([0.6778, 0.3753, 0.7416, 0.8120], dtype=torch.float64)]\n",
      "24 [tensor([[0.7126, 0.9482, 0.9998, 0.1869, 0.3878, 0.2132, 0.4729, 0.6440, 0.3182,\n",
      "         0.0168],\n",
      "        [0.8769, 0.7974, 0.4326, 0.5523, 0.1364, 0.5399, 0.4554, 0.7108, 0.6783,\n",
      "         0.5927],\n",
      "        [0.0172, 0.6426, 0.7915, 0.4452, 0.3237, 0.4100, 0.0157, 0.6517, 0.4677,\n",
      "         0.5877],\n",
      "        [0.1266, 0.3726, 0.4995, 0.6687, 0.8362, 0.6325, 0.9198, 0.6323, 0.2639,\n",
      "         0.0386]], dtype=torch.float64), tensor([0.9395, 0.6705, 0.6860, 0.7468], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 10)\n",
    "y = np.random.rand(100)\n",
    "\n",
    "dataset = FakeDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision Training\n",
    "\n",
    "Author: Chi-Liang Liu <https://liangtaiwan.github.io> Ref: https://github.com/NVIDIA/apex Using mixed precision to train your networks can be:\n",
    "\n",
    "+ 2-4x faster\n",
    "+ memory-efficient in only 3 lines of Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apex\n",
    "\n",
    "NVIDIA-maintained utilities to steamline mixed precision and distributed training in PyTorch. Some of the code here will be included in upstream PyTorch eventually. The intention of Apex is to make up-to-date utilities available to users as quickly as possible.\n",
    "\n",
    "#### Apex.amp\n",
    "\n",
    "Amp allows users to easily experiment with different pure and mixed precision modes. Commonly-used default modes are chosen by selecting an \"optimization level\" or **opt_level**; each **opt_level** establishes a set of properties that govern Amp's implementation of pure or mixed precision training. Finer-grained control of how a given **opt_level** behaves can be achieved by passing values for particular properties directly to *amp.initialize*. These manually specified values override the defaults established by the **opt_level**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fromfrom  apexapex  importimport  ampamp\n",
    "\n",
    "  # Declare model and optimizer as usual, with default (FP32) precision# Decla \n",
    "model = torch.nn.Linear(10, 100).cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Allow Amp to perform casts as required by the opt_level\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "...\n",
    "# loss.backward() becomes:\n",
    "with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "    scaled_loss.backward()\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
