{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# microexpression classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"DATA/microexpression/fer2013.csv\"\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
       "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
       "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
       "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
       "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pixels'] = df['pixels'].str.split(' ').apply(lambda x: np.array([np.uint8(i) for i in x]).reshape(48,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>pixels</th>\n",
       "      <th>Usage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[70, 80, 82, 72, 58, 58, 60, 63, 54, 58, 60, ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>[[151, 150, 147, 155, 148, 133, 111, 140, 170,...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[231, 212, 156, 164, 174, 138, 161, 173, 182,...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[[24, 32, 36, 30, 32, 23, 19, 20, 30, 41, 21, ...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>[[4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 15, 2...</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion                                             pixels     Usage\n",
       "0        0  [[70, 80, 82, 72, 58, 58, 60, 63, 54, 58, 60, ...  Training\n",
       "1        0  [[151, 150, 147, 155, 148, 133, 111, 140, 170,...  Training\n",
       "2        2  [[231, 212, 156, 164, 174, 138, 161, 173, 182,...  Training\n",
       "3        4  [[24, 32, 36, 30, 32, 23, 19, 20, 30, 41, 21, ...  Training\n",
       "4        6  [[4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 15, 2...  Training"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# divide dataset to train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 记得把原来的index重设，否则val、test的dataframe序号不连续，否则后面y_val转成torch.LongTensor时会报错\n",
    "def devide_x_y(my_df):\n",
    "    return my_df['pixels'].reset_index(drop=True), my_df['emotion'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别筛选出training、publictest、privatetest对应的行\n",
    "training = df['Usage']==\"Training\"\n",
    "publicTest = df['Usage'] == \"PublicTest\"\n",
    "privateTest = df[\"Usage\"] == \"PrivateTest\"\n",
    "\n",
    "# 读取对应行的数据\n",
    "train = df[training]\n",
    "public_t = df[publicTest]\n",
    "private_t = df[privateTest]\n",
    "\n",
    "# 分为 X 和 y\n",
    "train_x, train_y = devide_x_y(train)\n",
    "val_x, val_y = devide_x_y(public_t)\n",
    "test_x, test_y = devide_x_y(private_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 28709, 3589, 3589, 3589, 3589)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x), len(train_y), len(val_x), len(val_y), len(test_x), len(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画一个图出来看一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvFvnyVgAAIABJREFUeJztnW3MX1WZ7q+bvgFWKKUv9M2+WRUQaWNR0AoEMKAzgh/UjI4nnITIB88kTmbGEc9JJmeSc6J+GflwjnNCjmZQJ4PzFiFkjsfKqUwmmmKhLeI0fUEotH3aUvpGRbF9uubD8++k+1pX+7/5t/336VnXLyHtWtx777XX3qv7ua/nvu8VpRQYY9riovM9AGPM8PHCN6ZBvPCNaRAvfGMaxAvfmAbxwjemQbzwjWkQL3xjGuSMFn5E3BURmyNiW0Q8cLYGZYw5t8SgkXsRMQHAFgAfBrADwM8AfLqU8q+nOmby5Mnl0ksv7fRddFH3356IqI4bHR09bftNjLnTPn78eF8bBV//LW95S2UzefLkqo/vVc099ykbvr66D9XXbzzcBoAJEyZUfRMnTuy0J02adNbOPQg8R6+//nplc/To0U5bPWc1ZzzXx44d62ujnlnmeWTeT+7jax09ehSjo6N9X+KJ/QxOw/sAbCul/BIAIuIRAPcAOOXCv/TSS7Fq1apO31vf+tZOW70Mr732Wqe9f//+voNT5+GXT70g/BKr8xw8eLDTvuGGGyqbxYsXV30XX3xxp63+AeMX9Le//W3f6//617+ubFRfv/FMnTq1srnsssuqvhkzZnTas2bNqmz4uU6ZMqWymTZtWqetXnT+R0YtPJ6zjRs3Vja7d+8+7XkB4De/+U3Vd+jQoU77lVdeqWwOHDhw2vEA9fNQ7xW/e0eOHKlsuI/nbPv27dUxijP5UX8egJdPau/o9RljxjlnsvDVjxPVzzgRcX9ErIuIderrZYwZPmey8HcAWHBSez6AXWxUSnmolLKylLJS+b3GmOFzJj7+zwAsi4jFAHYC+D0AnzndAcePH698Vva9lA/HAobyja+44opOm/1XoPaXlZ/HeoLy1z7ykY902srHZxET0PfGZP5xfOONN970eZW4xj6lmjM1HtYClA33ZcROBfvC6tmzVrNixYrKZs2aNZ32zp07Kxv1zHiM6vo8b+qdYZS4yDqAej957vneM+I0cAYLv5RyLCL+AMD/BTABwLdKKb8Y9HzGmOFxJl98lFL+CcA/naWxGGOGhCP3jGmQM/riv1lGR0fx6quvdvrYR7n88sur4/h3wsrPYn9I/U6WfS/+fTgATJ8+vdO+4447Kpvly5d32sp/HTQ4hf1u5bPxuVUADR+XOU9GB1B9yhflOVHzwWNSgS9so/QMvj6/LwCwbNmyTnvz5s2VjbpX9rv59/pA/Q6r+eB7U3EWrC8pzYH1BL5W1sf3F9+YBvHCN6ZBvPCNaRAvfGMaZKji3rFjxypBjcUKJWhwYoJKXmBhJCMmvetd76ps7rzzzk77qquuqmxYBMuKe2ynklL4uEwyh4LvVQl3LAxl5kyRybxTghffvxJt+Twq2YfPrUTCq6++utP+4Q9/WNkosZcDZpQox3Ok7oNFSfUOZ0La+81ZNtvWX3xjGsQL35gG8cI3pkGG6uMDtY/Cvo4qjsHBOKo4BPuZyua6667rtG+++ebKZvbs2ac9r0L5dMrv53vP+MaZQKBM0ZHMccqfz+gJ6vo8J8rH5zEqmwx8fRXkM3PmzE577ty5lc3TTz9d9c2b1y0xoYKDuBCHSnbKwMdxoRKg9uFZ28riL74xDeKFb0yDeOEb0yBe+MY0yNCz8zhIggMylCh15ZVXdtos1AC1mLd06dLK5vrrr++0VVlsRglXg5Yk53vLlE9W88HCmRLleF4HHfPZQgWnZEpwZ0TKDJdcckmnvWTJksrmRz/6UdXH4tnChQsrm8OHD3famUxIFYjEIva+ffsqG85ezZTtVviLb0yDeOEb0yBe+MY0yNADeBj2dTK7sqjAhne84x2d9qJFiyob9vMG3cKKfTgVeKL8vEziTOY8TMZ/z1wrE2SjxqTGOEhlGOWvZp7HIKidjlTVppGRkU5b6UKsL6mgmkywEleK5mpVQJ0kNOj8+ItvTIN44RvTIF74xjSIF74xDTJ0cY8DGbjkdaZ8sgrg4SAfFSCR2Y6JBZ5Mxlq2pDHfmxJiztX+gupameotgx6X2Z5rkGw8FVDF86pEQrZRlZXUGHlbbHXuzBbYmTHynCkRm8/NQqLLaxtjTokXvjEN4oVvTIMM1ccvpVQ+Pfs1yvdhX1Bts8W+sQo8YV80kwCjgjrYRlWpyQS+KD2D+5QfzH5eJvBFjZFtMsk+QH3/mS3NVHVaRs1ZJomLbdQ21ZwkpPx5VV1n165dnbbyu/kdyehLmYAqNUaex4xupPAX35gG8cI3pkG88I1pEC98Yxpk6AE8GWGM4ewnlSHFfSqIgkUwJa4NUk47I8ApOyWmKWGKyZRv7rdVGVDfayYQR/W98cYblQ3fm9oaLSNMZbbZYtTzyMyreva/+tWvOm0l9maeB89ZZjwKvr4r8Bhj0njhG9MgfRd+RHwrIvZGxHMn9U2PiNURsbX35xWnO4cxZnyR8fH/CsD/APDtk/oeAPBEKeWrEfFAr/2lfieKiMof4mCH6dOnV8dxH1cqAXIJH5lqJZmglky1XOXDZfwxtlHVXHibMRV4wvfB2zwB9dwrX13NEW8zpq7PATPZ5BEms6UY+/1Ku+l3XkBXAs5sXZ25Hvv4SjvJJJH1q5581pJ0Sin/DGA/dd8D4OHe3x8G8PHU1Ywx44JBffzZpZQRAOj9WRfKM8aMW875r/Mi4n4A9wNnb3MEY8yZMehK3BMRcwCg9+feUxmWUh4qpawspawc1M8zxpxdBv3iPwbgXgBf7f35aOagiy66qBL3uHLOsmXLquPmz5/faatgkMw+8iyWZI5R4g6LWVy2+1Tn5nvP/ASkxDXO0FKiHJ87IyYpGxYSgTqIRN3rkSNHOm1VWYjnUc01X0sJaZltx/g4Jb6qe2XRWM0RP391HkZ9BDOZmf2yWc+auBcRfwPgpwDeGRE7IuI+jC34D0fEVgAf7rWNMRcIfb/4pZRPn+J/3X6Wx2KMGRJW24xpkKEm6UycOLHy6XmrK7V9MQfwDFqxlX2oTJKO8l/Zj1KBFqqP/TGlDWS22VIViBj2YVVVGHV9Rm3jdOjQoU5baQx8nJoPfo4qEIiPU775IPrO/v0cmlLflxqj8t/5GQ267RmPUeki/apIucquMeaUeOEb0yBe+MY0iBe+MQ0yVHFv0qRJ1dZFCxcu7LTV9lgscmT2cVeZcJnS2SyeKIFlz549nTYHqwC6nDQLU0qImTFjRqetgpVY3OMKRUA9bnUePi5bJpwzBg8fPlzZcJ8a486dOzttFn4BYOrUqZ12JlhKPdd+W08Beo44qEidm+dNvZ/8XmUqCWUEwEHxF9+YBvHCN6ZBvPCNaRAvfGMa5LxH7nEZLRVhlinBzYKKEvc4wkxFgbFQp0Q6jkpTEV9KFMxE/O3bt6+vTWZPdJ4PznAEaiFVRc6pveL4ekqEYjFNRbxx9FymrJZ6ZiwAqvHwmNXzUeIevzMqg3CQqNHMGDNkrq3wF9+YBvHCN6ZBvPCNaZCh+vgXXXRRtdUVB2SogBH2/TK+UCZjSwVjsL+ufNNMlp3yzdk/Vf4qb33FWzgBtQ6Q2dZpx44dfW1UAI0qZb5gwYK+NqwfqHvlIB8VCMWaj/Kx2V9nnx/I6URqazYuS64yOjOVczL71mds+mVvOjvPGHNKvPCNaRAvfGMaxAvfmAYZqrgXEZXIwmJEZq8yFXzBgooSCTNZVCx4KeGMxbUXXnihslEiS2Y/O0aJi5n92Fk4U6IUZ/mpe1XiImfaqcAfFkBZtASAkZGRTluV8GKx9eWXX65seF7VeDiAST171TeIkJwJqFKwIKzOw88+s2+fwl98YxrEC9+YBvHCN6ZBhurjl1KqQI5M8kJmiyT2s5QN+36Z86hAHA5OUX6WCgZhbUAF1fC9qqSl2267rdNWGkOmvDX3qYQcdR+ssagAJj7uxRdfrGx+8IMfdNqqlDdrHOqZsf+8YsWKvuNRc6bePT63CkTieVRjzATa8LPPBPRk1o/CX3xjGsQL35gG8cI3pkG88I1pkPMu7rF4ozK9MsIIizBKFGPhI7P/OJe7Bmoxa/v27ZWNCkTqt7c5ALz00kud9k033VTZrFq1qtNWAU0sXimRjp/FvHnzKhvetxAA5syZ02nPmjWrsuF5+/73v1/ZbNiwodNW1Y5YSL311lsrG2bv3r1VH4tgau+8TCUhJe7xuTOZgJnsUTWeQQN2GH/xjWkQL3xjGsQL35gGGaqPPzo6WiWPcGKG8nsze9ZngkrYp1dBLRzko/xO9r3mzp1b2Tz33HNVH4/pk5/8ZGXD11NVcXjcixcvrmw4KYUTcoA6oCgTiKPGpLQB9vuVDsGVfFQiD7N06dKqb/ny5Z32k08+Wdmwb56p9gPUelLGNx/Uf+c5ygT5DFKdCvAX35gm8cI3pkG88I1pkL4LPyIWRMSaiNgUEb+IiC/0+qdHxOqI2Nr7s/4FvDFmXJIR944B+ONSyjMR8VYAT0fEagD/EcATpZSvRsQDAB4A8KXTnWh0dLTa/imzRVIma4lFDRWcw+LNIOIJUFeqURVfrr322qqPM9S4Ag1Ql4ZWGWscIKKEOxbllHDFwVJqXtVxHNSjtp7iKj133XVXZcNBPirwhp/jLbfcUtkwd955Z9W3ZcuWTltVG1JCZiZbcxDUu8fPNfOeD7LtFpD44pdSRkopz/T+/hqATQDmAbgHwMM9s4cBfHygERhjhs6b8vEjYhGAFQDWAphdShkBxv5xAFDHbY4dc39ErIuIdeprbowZPumFHxFTAfwDgD8spRzuZ3+CUspDpZSVpZSVmRhmY8y5JxXAExGTMLbo/7qU8o+97j0RMaeUMhIRcwDUDhpx9OhR7Nq1q9PHVVwzWyWrIB/2h5QNn1v9Q8Q2ysfl4A/l96lkIw70Yb0DqJMwMgFNquIL+6tK8+BgIVXlVt0/B/pktja/5ppr+l5fbeXNmoeqTMzjUQFN69ev77QzWhKQ86EHqaCr4DGp95PHM0jVHiCn6geAbwLYVEr5i5P+12MA7u39/V4Aj6auaIw572S++B8E8B8A/DwiTuRR/mcAXwXwtxFxH4CXANTxp8aYcUnfhV9K+RcAp/p55/azOxxjzDBw5J4xDTLU7LzJkydXAg4LM0qcYNFJZZGxTUa4U6IMC2UqOIVtVFUUdX0WYlTgDQs8GVFIbanF11Jj5HMrYVWdm6sLqTHyHHFADwAsWbKk0965c2dlw9mcCg4EUsJqZksxFdQzSKZdRhDMCNSZa3sLLWNMGi98YxrEC9+YBhmqjz9lypSqggr768rPYj8zUz1F+T4cMKJ8KPazMltyq2upABE+d2Y7psx2zhmfUo0xk4CSGaOqKJy5D05uuuqqqyqbRYsWddrqXjPaDd+/Ok/GX1Ya1CCJO+oYHpOyyWwVl8FffGMaxAvfmAbxwjemQbzwjWmQoW+hxQIKi3mqnHVGPOHjlMCTOQ8HVqiMNQ7qUdlpKkCDhbJMFZYMmeAcJZpmsh6VuMf3q+ZVCX4MC6eqkhFnbyqxlQViNYc8R0rYVfOYqXjDx6k5y2Tw8fyrY3hes9l4jL/4xjSIF74xDeKFb0yDeOEb0yBDFfciohKGMtFsjBLTMlF5mUzAjFjCgpPK4FMCEwtT6j6YjOCk4OhGVearXxmnU12LRSf1zPh5qIxKRs0Zz62KVON5VWIjj1kJZ+r+WdzNlGRX79AgEXaZ8tqZDD6Fv/jGNIgXvjEN4oVvTIMM3cfvF6Ci/DP2oZSfxX6e8o3ZX1N+Hvuryjfjc6trqe2Y+D4yWzYp+Hqvv/56ZXPo0KHTtoHcVmCqj+dNVenh+1CBN/3Oe6o+hp+R0hwy2XnqWiqAi1HaQD8ymXcZG15P2S21/MU3pkG88I1pEC98YxrEC9+YBhmquKdgMSJTqlplfnHAiBKc+FoZ4UYJPixUqWsNup9aZg/AfhmOAHDw4MFOm4OOgHqf+0wADVAH4yjhjvuUDc+bug8ek3pmfG/q/eCApqyQyEFWquxbJoBokNLZymaQ8usKf/GNaRAvfGMaxAvfmAYZegAPB1tkgh/Yj1F+Dfv4vK86UPtiqtoP+88qyIa1gsxWXMpO+aLs16n54eNUAg77+OzPqz51LXVvfP+ZICf1zHhulf88SCnzV155pbLZt2/fac97KtjHH2RLLUWmko+6Fo/bFXiMMWm88I1pEC98YxrEC9+YBhl6AE+/rK1MqepBg1o4GGTQfeVZmMmIhEAuqCVzHg5YefXVVysbFrxUcA6LhGo8mUw3Ndcs+CnhTgmw/a6f2XPuhRdeqGw4g1Hdq3qveN4y85iZs4y4mMm0y5T/VviLb0yDeOEb0yB9F35EXBwRT0XExoj4RUT8ea9/cUSsjYitEfG9iOj/c6sxZlyQ8fHfAHBbKeVIREwC8C8R8X8A/BGAr5dSHomI/wXgPgB/eboTqQCeTEAPB3pkfGrld7JPr/wsTkrJ+JSDBlFkjlNBPuzjK5+Sg3p27txZ2fA8HjhwoLJR88hVea6++urKZtasWZ22ShJiPWfOnDmVDesAKkmH53HTpk2VDb8fGX0FyOlLGV0os6UYM+h7laHvF7+McUKZmdT7rwC4DcDf9/ofBvDxczJCY8xZJ+XjR8SEiNgAYC+A1QCeB3CwlHLin7YdAOadmyEaY842qYVfShktpSwHMB/A+wDUP9uN/RRQERH3R8S6iFinikIaY4bPm1L1SykHAfwYwI0ApkXECSd5PoBdpzjmoVLKylLKSlXUwRgzfPqKexExE8DRUsrBiLgEwB0AvgZgDYBPAHgEwL0AHs1ckAN4Mts4sViigihY4FJiCgtVStxjMS+TeaZQgRR8rswWSZl97VXm3ZYtWzrtn/zkJ5XNiy++2Gnv3r27slFjnD9/fqe9cePGyobvVYlpt9xyy2nPC+QCeDgbb+vWrZUNvzPqPcs8j0zWpXr2g1TKyQjLg26hlVH15wB4OCImYOwnhL8tpTweEf8K4JGI+G8A1gP45kAjMMYMnb4Lv5TyLIAVov+XGPP3jTEXGI7cM6ZBhpqkU0qpfFYOqlH+UWZbqwyZra/4WippiH1B5Rsq/4z71HGZLZc5gEb5q0uXLu20VVUa9k3f/va3VzYq8Iar+6iAqkWLFnXaH/rQhyob9ukzz0PNKwfs8PiA+l6VdqL8ZdYGlA3Pf2ab7kG23QIGf/er85yVsxhjLii88I1pEC98YxrEC9+YBhm6uJcpu8z0y+gDBgtkyGRRZUocK0EyI/hlgkiUuMhbWKk5nDevmzpx6623VjYs+CmRTpXu5lLVKoBo1apVnbbK4OOMQfU8eP6V2Pjss89WfUwm6CpDJjhH2XCfemZ8r8rG4p4xZmC88I1pEC98YxpkqD7+8ePHKx+NfRa1ZVWmYiyjfONMFRT235VPlanAk9kiKXN9pQNwFRrlC2a0FNYKLrvssspm9uzZVR9X11HH8bhVlSC+fmaL9M2bN1c227Zt67Qz1ZOzzycTwDPIFtgKfkbZbb4GwV98YxrEC9+YBvHCN6ZBvPCNaZChb6HFZDLmWCwZdF/5TClvDljJnEeJMEqAZPFGlYpmUfLw4cOVDQfaKAGSx63qHfJ41Lyqyjl8v+r+OftNbfPFpbPV9fleVSWhjGiqxEVm0HLWme25uE+NORMYxvCz9xZaxphT4oVvTIN44RvTIEMP4GG/ln0U5Yux35vZ4lidh6+VCXxRvjqPR/lryu/mManjeH4OHTpU2bAvOmhSCPvYal6V3833xoE4QO2v7t+/v6+NKr++du3aTpsrA6vrK+2E515t0a2OYxYvXlz1cQLS9u3bK5uXXnqp0+YqSkD9XmX0pUHxF9+YBvHCN6ZBvPCNaRAvfGMaZOgVeFSVl36wyKEEDu5T5ZP5PCqDL7OFFWd/ZcpkA7XApoJqMvOTqUjEoqQSO48cOdJpq6y2uXPnVn0Z4TAjQLKNEjt/+tOfdtrqPlgkzGTQ8b0Durz4e97znk575syZlc1nPvOZ014LAFavXt1pf+Mb36hsRkZGOu1BsvOyZbv9xTemQbzwjWkQL3xjGsQL35gGGaq4Nzo6Kssjn4wSmDiiSok3mX3tOTItUzJLCU4swGX211NjVOfm0tVqzzvOIFQZfLx/nBLF+PpKTFq2bFnVx3OrIt5uuummTvvKK6+sbLhvx44dlQ1H6qkoQb4Pda9XXXVVp3399ddXNp/73OeqPi7d/Z3vfKey4VLiqhTZjTfe2GmrkuQPPvhgp62eK7/DLD5ny375i29Mg3jhG9MgXvjGNMjQs/PYP834Z5xJpfY/58CFTBUUpQNkgiZ4jGrMytdif0wFtcyZM6fTVoEmnP114MCByoaPU8FC3Key45TfPW3atE778ssvr2w40EWVTec5Wr9+fWXDmpA6D78fn/rUpyqbm2+++bTjA/Qc8fuq3o+vfOUrnfbu3bsrG9Ym1PUXLFjQafMWY6rviiuu6LSz2Xv+4hvTIF74xjRIeuFHxISIWB8Rj/faiyNibURsjYjvRUT9s7UxZlzyZr74XwCw6aT21wB8vZSyDMABAPedzYEZY84dKXEvIuYD+B0A/x3AH8WYKnUbgBNpSQ8D+K8A/vJ05ymlVGIeCypKPGGbzH52qmQV26hMJhYAlQDHNiqAJRNIocovcfCHyhhTe90zPEcqcIpFUjX3KhiFxbxMdp66PguQGzdurGw4y1A9s9tvv73Tfv/731/Z8L0+9thjlY26/t69ezttVYqMBUBVSpwFUfV+snCp3g+e1y1btnTaKptUkf3iPwjgTwGcmPUrARwspZxYxTsAzEueyxhznum78CPidwHsLaU8fXK3MJW7EUTE/RGxLiLWZTY1MMacezI/6n8QwN0R8VEAFwO4DGM/AUyLiIm9r/58ALvUwaWUhwA8BABTp04dbKsSY8xZpe/CL6V8GcCXASAibgXwJ6WU34+IvwPwCQCPALgXwKOJc1XBLuwfKX+Zgx0y+58rVFAPw36uOob990yVHqAOtlClu3l+VEUeHqNKCOKAEQ4OAeoEHJVspOa1X6IVUM+bCjJi/3Tz5s2VDd/rddddV9ksWbKk77U2bdrUaavAJOWb87NVyTWM0oX4Pc8kiLEuAAB33313p83BQt/+9rf7jg84s9/jfwljQt82jPn83zyDcxljhsibCtktpfwYwI97f/8lgPed/SEZY841jtwzpkG88I1pkKFm52VQe6xxMIoSPVgsUaIcZ3Flfr2oglpYqFFCXmbvPD6PIrOPugoW4uotu3bVv3Rh4VCdR831jBkzOm1VFYdFMfVcuXT27Nmz+17/hhtuqGw40EXdhwrOYa655pqqb9u2bZ22Eja5uk9mv0OVYcrCtnr21157bafNAV5KoFX4i29Mg3jhG9MgXvjGNMjQK/Cwj8Q+ifKN2T9UNuxnq8AXPk4loLA2oKqyZCr5qEoxPCaV3JIJDuIAFRWwwskkKrmEq7kov1NV2f3ABz7QaSvfnOfkqaeeqmyeeeaZTvvd7353ZfOxj32s02Z/GqjnTD173p9e6TscYAXUW4ipyrdcCVnNB+tLHLwE1M9aXevJJ5/stDl46Wwn6Rhj/j/CC9+YBvHCN6ZBvPCNaZChintTpkzBO9/5zk4fB1aoEs8sXqnAhlmzZnXaSjjLlPLmYBQVDMIBIyrLLlOmO7OvvboPDvRQASOZrZX4XrNbgany0QzP9eOPP17ZsOD2+c9/vrLhbDwl7vEcPf/885UNP2sV0LRnz56qj6/HYhpQPzN1nkWLFnXaqiQ5Zyeqd4if9YYNGzptJWwq/MU3pkG88I1pEC98YxpkqD7+pEmTquAGDhBhvw+ofU+V8MGBQcqH4sAb5fdyFRSVgMP+ogqOUb4WBwypc7MNB34AdeIK6yZArjot34fSHDLVgpUOsWbNmk775Zdfrmy++MUvdtqf/exn+45RVbfhrcVV0tDixYs7bd7+GtDVi3m7b97iDKgDn9RWYHz/CxcurGw4gElpFax3ZbZDV/iLb0yDeOEb0yBe+MY0iBe+MQ0yVHFvdHS0EuY48EaVL+atllQZZBZCRkZGKhsWCVXmHYtySiTMlPJWwTB8PWXDwpTK9GLhTgXZcJCRGjOLckqkU2IRVw5SQtnatWs7bRWMwkKZCgziLEf17F988cW+NlxdZ+nSpZWNys7jAKL58+dXNply4yycqkxIFm1VRSB+Rrw2sviLb0yDeOEb0yBe+MY0yHmvssv+ovKpWQfgNlAHaHBVFKCumKpsOAFGVUHhwBsV1KH8d/a7VSASB6ioKkFso/xw9qlV4AsHHikbtaUZ+5U///nPKxveokr5ot/97nc7bfXs3/a2t3Xaal4zW5vzM1Kah3of+Bmpykrsv2e2X1fbn3NFJFUp+r3vfW+nzbqASj5S+ItvTIN44RvTIF74xjSIF74xDRIq+OOcXSziFQDbAcwAsK+P+XjjQhwzcGGO22MenIWllJn9jIa68P/9ohHrSikrh37hM+BCHDNwYY7bYz73+Ed9YxrEC9+YBjlfC/+h83TdM+FCHDNwYY7bYz7HnBcf3xhzfvGP+sY0yNAXfkTcFRGbI2JbRDww7OtniIhvRcTeiHjupL7pEbE6Irb2/qyTt88jEbEgItZExKaI+EVEfKHXP27HHREXR8RTEbGxN+Y/7/Uvjoi1vTF/LyLqoPXzTERMiIj1EfF4rz3ux3wyQ134ETEBwP8E8BEA1wD4dETU1QbOP38F4C7qewDAE6WUZQCe6LXHE8cA/HEp5WoANwL4T725Hc/jfgPAbaWU6wEsB3BXRNwI4GsAvt4b8wEA953HMZ6KLwDYdFL7QhjzvzPsL/77AGwrpfyylPJbAI8AuGfIY+hLKeWfAXDq3D0AHu79/WEAHx/qoPpQShkppTzT+/trGHsp52Ecj7uMcSJtb1LvvwLgNgB/3+sfV2MGgIiYD+B3APzvXju5sEdqAAAB0ElEQVQwzsfMDHvhzwNwcoHxHb2+C4HZpZQRYGyRAahzg8cJEbEIwAoAazHOx937kXkDgL0AVgN4HsDBUsqJzffG4zvyIIA/BXAi//ZKjP8xdxj2wq8Tpcf+hTdniYiYCuAfAPxhKaUuJjDOKKWMllKWA5iPsZ8Ir1Zmwx3VqYmI3wWwt5Ty9MndwnTcjFkx7EIcOwAsOKk9H0CucsD5Z09EzCmljETEHIx9ocYVETEJY4v+r0sp/9jrHvfjBoBSysGI+DHG9IlpETGx9wUdb+/IBwHcHREfBXAxgMsw9hPAeB5zxbC/+D8DsKyngE4G8HsAHhvyGAblMQD39v5+L4BHz+NYKnp+5jcBbCql/MVJ/2vcjjsiZkbEtN7fLwFwB8a0iTUAPtEzG1djLqV8uZQyv5SyCGPv7/8rpfw+xvGYJaWUof4H4KMAtmDMl/svw75+cox/A2AEwFGM/ZRyH8b8uCcAbO39Of18j5PGvApjP14+C2BD77+PjudxA3gPgPW9MT8H4M96/UsAPAVgG4C/AzDlfI/1FOO/FcDjF9KYT/znyD1jGsSRe8Y0iBe+MQ3ihW9Mg3jhG9MgXvjGNIgXvjEN4oVvTIN44RvTIP8GhAeK/H0M050AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img_test = train_x[0]\n",
    "plt.figure()\n",
    "plt.imshow(img_test,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class MicroExpreDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            # label needs to be a LongTensor\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else: return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_set = MicroExpreDataset(train_x, train_y, train_transform)\n",
    "val_set = MicroExpreDataset(val_x, val_y, test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        # super继承父类\n",
    "        super(Classifier, self).__init__()\n",
    "        \n",
    "        # conv2d( in_channels, out_channels, kernel_size, stide, padding)\n",
    "        # input shape: [1, 48, 48]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 2, 1, 1), # [64, 49, 49]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), #[64, 25, 25]\n",
    "            \n",
    "            nn.Conv2d(64, 128, 2, 1, 1), # [128, 26, 26]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0), # [128, 13, 13]\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 13, 13]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), # [256, 7, 7]\n",
    "            \n",
    "            nn.Conv2d(256, 256, 3, 1, 1), #[256, 7, 7]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), #[256, 4, 4]        \n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2304, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Linear(256, 7)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouzy/softwares/Python-3.7.3/install/lib/python3.7/site-packages/torch/cuda/__init__.py:87: UserWarning: \n",
      "    Found GPU1 Quadro K600 which is of cuda capability 3.0.\n",
      "    PyTorch no longer supports this GPU because it is too old.\n",
      "    The minimum cuda capability that we support is 3.5.\n",
      "    \n",
      "  warnings.warn(old_gpu_warn % (d, name, major, capability[1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TITAN RTX'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练30步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] 12.03 sec(s) Train Acc: 0.328573 Loss: 0.013148 | Val Acc: 0.426024 loss: 0.012188\n",
      "[002/030] 11.78 sec(s) Train Acc: 0.478143 Loss: 0.010555 | Val Acc: 0.488994 loss: 0.010460\n",
      "[003/030] 11.24 sec(s) Train Acc: 0.533979 Loss: 0.009564 | Val Acc: 0.525216 loss: 0.009936\n",
      "[004/030] 11.79 sec(s) Train Acc: 0.563482 Loss: 0.008975 | Val Acc: 0.561159 loss: 0.009435\n",
      "[005/030] 11.62 sec(s) Train Acc: 0.585322 Loss: 0.008572 | Val Acc: 0.577320 loss: 0.009204\n",
      "[006/030] 11.28 sec(s) Train Acc: 0.606569 Loss: 0.008150 | Val Acc: 0.577041 loss: 0.008942\n",
      "[007/030] 11.50 sec(s) Train Acc: 0.618761 Loss: 0.007891 | Val Acc: 0.588465 loss: 0.008672\n",
      "[008/030] 11.54 sec(s) Train Acc: 0.639347 Loss: 0.007554 | Val Acc: 0.567846 loss: 0.009275\n",
      "[009/030] 11.66 sec(s) Train Acc: 0.655509 Loss: 0.007229 | Val Acc: 0.580942 loss: 0.008921\n",
      "[010/030] 11.94 sec(s) Train Acc: 0.668083 Loss: 0.006954 | Val Acc: 0.604068 loss: 0.008808\n",
      "[011/030] 12.46 sec(s) Train Acc: 0.685012 Loss: 0.006616 | Val Acc: 0.617721 loss: 0.008682\n",
      "[012/030] 12.39 sec(s) Train Acc: 0.704762 Loss: 0.006281 | Val Acc: 0.585400 loss: 0.009206\n",
      "[013/030] 12.41 sec(s) Train Acc: 0.717301 Loss: 0.006026 | Val Acc: 0.592923 loss: 0.009559\n",
      "[014/030] 12.42 sec(s) Train Acc: 0.730781 Loss: 0.005714 | Val Acc: 0.610198 loss: 0.009014\n",
      "[015/030] 12.31 sec(s) Train Acc: 0.744610 Loss: 0.005421 | Val Acc: 0.600167 loss: 0.009234\n",
      "[016/030] 12.24 sec(s) Train Acc: 0.761085 Loss: 0.005049 | Val Acc: 0.612148 loss: 0.009197\n",
      "[017/030] 12.28 sec(s) Train Acc: 0.769201 Loss: 0.004888 | Val Acc: 0.605461 loss: 0.010390\n",
      "[018/030] 11.79 sec(s) Train Acc: 0.791982 Loss: 0.004491 | Val Acc: 0.614377 loss: 0.009614\n",
      "[019/030] 12.38 sec(s) Train Acc: 0.802814 Loss: 0.004215 | Val Acc: 0.621900 loss: 0.009871\n",
      "[020/030] 12.93 sec(s) Train Acc: 0.817096 Loss: 0.003962 | Val Acc: 0.605183 loss: 0.010428\n",
      "[021/030] 11.68 sec(s) Train Acc: 0.825978 Loss: 0.003768 | Val Acc: 0.600446 loss: 0.010582\n",
      "[022/030] 11.79 sec(s) Train Acc: 0.834965 Loss: 0.003572 | Val Acc: 0.605461 loss: 0.010803\n",
      "[023/030] 12.41 sec(s) Train Acc: 0.852416 Loss: 0.003229 | Val Acc: 0.601003 loss: 0.012003\n",
      "[024/030] 12.33 sec(s) Train Acc: 0.857153 Loss: 0.003141 | Val Acc: 0.606576 loss: 0.011445\n",
      "[025/030] 12.55 sec(s) Train Acc: 0.872827 Loss: 0.002829 | Val Acc: 0.620228 loss: 0.012345\n",
      "[026/030] 12.67 sec(s) Train Acc: 0.880874 Loss: 0.002638 | Val Acc: 0.622458 loss: 0.011975\n",
      "[027/030] 12.69 sec(s) Train Acc: 0.879132 Loss: 0.002663 | Val Acc: 0.600167 loss: 0.012180\n",
      "[028/030] 12.63 sec(s) Train Acc: 0.879863 Loss: 0.002595 | Val Acc: 0.612984 loss: 0.012249\n",
      "[029/030] 12.57 sec(s) Train Acc: 0.896269 Loss: 0.002296 | Val Acc: 0.618835 loss: 0.012378\n",
      "[030/030] 12.61 sec(s) Train Acc: 0.897210 Loss: 0.002303 | Val Acc: 0.612427 loss: 0.012888\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "model = Classifier().cuda()\n",
    "\n",
    "# classification task, so use crossEntropyLoss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# optimizer use Adam\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epoch = 30\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    \n",
    "    # open train mode\n",
    "    model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # set gradient to zero, otherwise it will be added each time\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # train model to get the predict possibility. Actually, it is to call the forward function\n",
    "        train_pred = model(data[0].cuda()) # data[0] is x\n",
    "        batch_loss = loss(train_pred, data[1].cuda()) # data[1] is y\n",
    "        \n",
    "        # use backward to calculate the gradient for each parameter\n",
    "        batch_loss.backward()\n",
    "        # update the parameters by optimizer\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss += batch_loss.item()\n",
    "        \n",
    "    # turn model to eval mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            val_pred = model(data[0].cuda())\n",
    "            batch_loss = loss(val_pred, data[1].cuda())\n",
    "            \n",
    "            val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            val_loss += batch_loss.item()\n",
    "        \n",
    "        # print out the results\n",
    "        print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "            (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "             train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示模型参数结构，前面算FC时，尺寸和我推算的不一致\n",
    "\n",
    "打印后可知，原来是maxpool的时候并没有补数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary(my_model, my_input_size):\n",
    "    from collections import OrderedDict\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    import torch\n",
    "    from torch.autograd import Variable\n",
    "    import torch.nn.functional as F\n",
    "    from torch import nn\n",
    "\n",
    "\n",
    "    def get_names_dict(model):\n",
    "        \"\"\"\n",
    "        Recursive walk to get names including path\n",
    "        \"\"\"\n",
    "        names = {}\n",
    "        def _get_names(module, parent_name=''):\n",
    "            for key, module in module.named_children():\n",
    "                name = parent_name + '.' + key if parent_name else key\n",
    "                names[name]=module\n",
    "                if isinstance(module, torch.nn.Module):\n",
    "                    _get_names(module, parent_name=name)\n",
    "        _get_names(model)\n",
    "        return names\n",
    "\n",
    "\n",
    "    def torch_summarize_df(input_size, model, weights=False, input_shape=True, nb_trainable=False):\n",
    "        \"\"\"\n",
    "        Summarizes torch model by showing trainable parameters and weights.\n",
    "        \n",
    "        author: wassname\n",
    "        url: https://gist.github.com/wassname/0fb8f95e4272e6bdd27bd7df386716b7\n",
    "        license: MIT\n",
    "        \n",
    "        Modified from:\n",
    "        - https://github.com/pytorch/pytorch/issues/2001#issuecomment-313735757\n",
    "        - https://gist.github.com/wassname/0fb8f95e4272e6bdd27bd7df386716b7/\n",
    "        \n",
    "        Usage:\n",
    "            import torchvision.models as models\n",
    "            model = models.alexnet()\n",
    "            df = torch_summarize_df(input_size=(3, 224,224), model=model)\n",
    "            print(df)\n",
    "            \n",
    "            #              name class_name        input_shape       output_shape  nb_params\n",
    "            # 1     features=>0     Conv2d  (-1, 3, 224, 224)   (-1, 64, 55, 55)      23296#(3*11*11+1)*64\n",
    "            # 2     features=>1       ReLU   (-1, 64, 55, 55)   (-1, 64, 55, 55)          0\n",
    "            # ...\n",
    "        \"\"\"\n",
    "\n",
    "        def register_hook(module):\n",
    "            def hook(module, input, output):\n",
    "                name = ''\n",
    "                for key, item in names.items():\n",
    "                    if item == module:\n",
    "                        name = key\n",
    "                #<class 'torch.nn.modules.conv.Conv2d'>\n",
    "                class_name = str(module.__class__).split('.')[-1].split(\"'\")[0]\n",
    "                module_idx = len(summary)\n",
    "\n",
    "                m_key = module_idx + 1\n",
    "\n",
    "                summary[m_key] = OrderedDict()\n",
    "                summary[m_key]['name'] = name\n",
    "                summary[m_key]['class_name'] = class_name\n",
    "                if input_shape:\n",
    "                    summary[m_key][\n",
    "                        'input_shape'] = (-1, ) + tuple(input[0].size())[1:]\n",
    "                summary[m_key]['output_shape'] = (-1, ) + tuple(output.size())[1:]\n",
    "                if weights:\n",
    "                    summary[m_key]['weights'] = list(\n",
    "                        [tuple(p.size()) for p in module.parameters()])\n",
    "\n",
    "    #             summary[m_key]['trainable'] = any([p.requires_grad for p in module.parameters()])\n",
    "                if nb_trainable:\n",
    "                    params_trainable = sum([torch.LongTensor(list(p.size())).prod() for p in module.parameters() if p.requires_grad])\n",
    "                    summary[m_key]['nb_trainable'] = params_trainable\n",
    "                params = sum([torch.LongTensor(list(p.size())).prod() for p in module.parameters()])\n",
    "                summary[m_key]['nb_params'] = params\n",
    "                \n",
    "\n",
    "            if  not isinstance(module, nn.Sequential) and \\\n",
    "                not isinstance(module, nn.ModuleList) and \\\n",
    "                not (module == model):\n",
    "                hooks.append(module.register_forward_hook(hook))\n",
    "                \n",
    "        # Names are stored in parent and path+name is unique not the name\n",
    "        names = get_names_dict(model)\n",
    "\n",
    "        # check if there are multiple inputs to the network\n",
    "        if isinstance(input_size[0], (list, tuple)):\n",
    "            x = [Variable(torch.rand(1, *in_size)) for in_size in input_size]\n",
    "        else:\n",
    "            x = Variable(torch.rand(1, *input_size))\n",
    "\n",
    "        if next(model.parameters()).is_cuda:\n",
    "            x = x.cuda()\n",
    "\n",
    "        # create properties\n",
    "        summary = OrderedDict()\n",
    "        hooks = []\n",
    "\n",
    "        # register hook\n",
    "        model.apply(register_hook)\n",
    "\n",
    "        # make a forward pass\n",
    "        model(x)\n",
    "\n",
    "        # remove these hooks\n",
    "        for h in hooks:\n",
    "            h.remove()\n",
    "\n",
    "        # make dataframe\n",
    "        df_summary = pd.DataFrame.from_dict(summary, orient='index')\n",
    "\n",
    "        return df_summary\n",
    "\n",
    "\n",
    "    # Test on alexnet\n",
    "    import torchvision.models as models\n",
    "    df = torch_summarize_df(input_size=my_input_size, model=my_model)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name   class_name        input_shape       output_shape        nb_params\n",
      "1    cnn.0       Conv2d    (-1, 1, 48, 48)   (-1, 64, 49, 49)      tensor(320)\n",
      "2    cnn.1  BatchNorm2d   (-1, 64, 49, 49)   (-1, 64, 49, 49)      tensor(128)\n",
      "3    cnn.2         ReLU   (-1, 64, 49, 49)   (-1, 64, 49, 49)                0\n",
      "4    cnn.3    MaxPool2d   (-1, 64, 49, 49)   (-1, 64, 24, 24)                0\n",
      "5    cnn.4       Conv2d   (-1, 64, 24, 24)  (-1, 128, 25, 25)    tensor(32896)\n",
      "6    cnn.5  BatchNorm2d  (-1, 128, 25, 25)  (-1, 128, 25, 25)      tensor(256)\n",
      "7    cnn.6         ReLU  (-1, 128, 25, 25)  (-1, 128, 25, 25)                0\n",
      "8    cnn.7    MaxPool2d  (-1, 128, 25, 25)  (-1, 128, 12, 12)                0\n",
      "9    cnn.8       Conv2d  (-1, 128, 12, 12)  (-1, 256, 12, 12)   tensor(295168)\n",
      "10   cnn.9  BatchNorm2d  (-1, 256, 12, 12)  (-1, 256, 12, 12)      tensor(512)\n",
      "11  cnn.10         ReLU  (-1, 256, 12, 12)  (-1, 256, 12, 12)                0\n",
      "12  cnn.11    MaxPool2d  (-1, 256, 12, 12)    (-1, 256, 6, 6)                0\n",
      "13  cnn.12       Conv2d    (-1, 256, 6, 6)    (-1, 256, 6, 6)   tensor(590080)\n",
      "14  cnn.13  BatchNorm2d    (-1, 256, 6, 6)    (-1, 256, 6, 6)      tensor(512)\n",
      "15  cnn.14         ReLU    (-1, 256, 6, 6)    (-1, 256, 6, 6)                0\n",
      "16  cnn.15    MaxPool2d    (-1, 256, 6, 6)    (-1, 256, 3, 3)                0\n",
      "17    fc.0       Linear         (-1, 2304)          (-1, 512)  tensor(1180160)\n",
      "18    fc.1         ReLU          (-1, 512)          (-1, 512)                0\n",
      "19    fc.2       Linear          (-1, 512)          (-1, 256)   tensor(131328)\n",
      "20    fc.3       Linear          (-1, 256)            (-1, 7)     tensor(1799)\n"
     ]
    }
   ],
   "source": [
    "show_summary(model, (1,48,48))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 调优测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把train的过程写到函数里，方便后面反复调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(my_model):\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "    model = my_model.cuda()\n",
    "\n",
    "    # classification task, so use crossEntropyLoss\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    # optimizer use Adam\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epoch = 30\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        # open train mode\n",
    "        model.train()\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # set gradient to zero, otherwise it will be added each time\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # train model to get the predict possibility. Actually, it is to call the forward function\n",
    "            train_pred = model(data[0].cuda()) # data[0] is x\n",
    "            batch_loss = loss(train_pred, data[1].cuda()) # data[1] is y\n",
    "\n",
    "            # use backward to calculate the gradient for each parameter\n",
    "            batch_loss.backward()\n",
    "            # update the parameters by optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "            train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "        # turn model to eval mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(val_loader):\n",
    "                val_pred = model(data[0].cuda())\n",
    "                batch_loss = loss(val_pred, data[1].cuda())\n",
    "\n",
    "                val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "                val_loss += batch_loss.item()\n",
    "\n",
    "            # print out the results\n",
    "            print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "                (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "                 train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型中加上dropout\n",
    "\n",
    "可以看到过拟合没有那么严重了，但val的acc还是没有提升上来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Classifier_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        # super继承父类\n",
    "        super(Classifier_1, self).__init__()\n",
    "        \n",
    "        # conv2d( in_channels, out_channels, kernel_size, stide, padding)\n",
    "        # input shape: [1, 48, 48]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 2, 1, 1), # [64, 49, 49]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), #[64, 25, 25]\n",
    "            \n",
    "            nn.Conv2d(64, 128, 2, 1, 1), # [128, 26, 26]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0), # [128, 13, 13]\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 13, 13]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), # [256, 7, 7]\n",
    "            \n",
    "            nn.Conv2d(256, 256, 3, 1, 1), #[256, 7, 7]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), #[256, 4, 4]        \n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2304, 512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 7)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/300] 11.84 sec(s) Train Acc: 0.307012 Loss: 0.013331 | Val Acc: 0.390917 loss: 0.012439\n",
      "[002/300] 11.64 sec(s) Train Acc: 0.446341 Loss: 0.011215 | Val Acc: 0.477292 loss: 0.010708\n",
      "[003/300] 10.34 sec(s) Train Acc: 0.487896 Loss: 0.010351 | Val Acc: 0.510170 loss: 0.010196\n",
      "[004/300] 10.30 sec(s) Train Acc: 0.520464 Loss: 0.009827 | Val Acc: 0.496517 loss: 0.010735\n",
      "[005/300] 10.31 sec(s) Train Acc: 0.537671 Loss: 0.009498 | Val Acc: 0.557258 loss: 0.009401\n",
      "[006/300] 10.30 sec(s) Train Acc: 0.553764 Loss: 0.009146 | Val Acc: 0.548064 loss: 0.009361\n",
      "[007/300] 10.27 sec(s) Train Acc: 0.573653 Loss: 0.008861 | Val Acc: 0.579827 loss: 0.009006\n",
      "[008/300] 10.24 sec(s) Train Acc: 0.587203 Loss: 0.008614 | Val Acc: 0.566174 loss: 0.009235\n",
      "[009/300] 10.30 sec(s) Train Acc: 0.600300 Loss: 0.008362 | Val Acc: 0.551964 loss: 0.009816\n",
      "[010/300] 10.37 sec(s) Train Acc: 0.613745 Loss: 0.008110 | Val Acc: 0.579549 loss: 0.008896\n",
      "[011/300] 10.35 sec(s) Train Acc: 0.620851 Loss: 0.007870 | Val Acc: 0.583171 loss: 0.009049\n",
      "[012/300] 10.34 sec(s) Train Acc: 0.637291 Loss: 0.007628 | Val Acc: 0.585678 loss: 0.009142\n",
      "[013/300] 10.37 sec(s) Train Acc: 0.642481 Loss: 0.007468 | Val Acc: 0.590972 loss: 0.009107\n",
      "[014/300] 10.34 sec(s) Train Acc: 0.654429 Loss: 0.007255 | Val Acc: 0.596545 loss: 0.008633\n",
      "[015/300] 10.35 sec(s) Train Acc: 0.663868 Loss: 0.007034 | Val Acc: 0.593759 loss: 0.008878\n",
      "[016/300] 10.33 sec(s) Train Acc: 0.676617 Loss: 0.006761 | Val Acc: 0.607412 loss: 0.008973\n",
      "[017/300] 10.31 sec(s) Train Acc: 0.685848 Loss: 0.006608 | Val Acc: 0.605461 loss: 0.009072\n",
      "[018/300] 10.32 sec(s) Train Acc: 0.698770 Loss: 0.006357 | Val Acc: 0.606297 loss: 0.009447\n",
      "[019/300] 10.32 sec(s) Train Acc: 0.707722 Loss: 0.006149 | Val Acc: 0.614656 loss: 0.009399\n",
      "[020/300] 10.32 sec(s) Train Acc: 0.718276 Loss: 0.005959 | Val Acc: 0.615213 loss: 0.009372\n",
      "[021/300] 10.68 sec(s) Train Acc: 0.730189 Loss: 0.005687 | Val Acc: 0.603232 loss: 0.009328\n",
      "[022/300] 10.32 sec(s) Train Acc: 0.739838 Loss: 0.005479 | Val Acc: 0.613541 loss: 0.009737\n",
      "[023/300] 10.33 sec(s) Train Acc: 0.745481 Loss: 0.005354 | Val Acc: 0.600167 loss: 0.009796\n",
      "[024/300] 10.33 sec(s) Train Acc: 0.755164 Loss: 0.005185 | Val Acc: 0.616328 loss: 0.010022\n",
      "[025/300] 10.33 sec(s) Train Acc: 0.762444 Loss: 0.004984 | Val Acc: 0.607412 loss: 0.010958\n",
      "[026/300] 10.32 sec(s) Train Acc: 0.776795 Loss: 0.004755 | Val Acc: 0.613263 loss: 0.011915\n",
      "[027/300] 10.37 sec(s) Train Acc: 0.783169 Loss: 0.004622 | Val Acc: 0.611591 loss: 0.011068\n",
      "[028/300] 10.33 sec(s) Train Acc: 0.786826 Loss: 0.004519 | Val Acc: 0.619114 loss: 0.011842\n",
      "[029/300] 10.34 sec(s) Train Acc: 0.795883 Loss: 0.004357 | Val Acc: 0.610755 loss: 0.011192\n",
      "[030/300] 10.27 sec(s) Train Acc: 0.801735 Loss: 0.004197 | Val Acc: 0.621343 loss: 0.011542\n",
      "[031/300] 10.34 sec(s) Train Acc: 0.814553 Loss: 0.003989 | Val Acc: 0.620507 loss: 0.012200\n",
      "[032/300] 10.35 sec(s) Train Acc: 0.813055 Loss: 0.003982 | Val Acc: 0.615770 loss: 0.011663\n",
      "[033/300] 10.37 sec(s) Train Acc: 0.823923 Loss: 0.003766 | Val Acc: 0.613263 loss: 0.012323\n",
      "[034/300] 10.33 sec(s) Train Acc: 0.827371 Loss: 0.003712 | Val Acc: 0.624129 loss: 0.012311\n",
      "[035/300] 10.35 sec(s) Train Acc: 0.832248 Loss: 0.003594 | Val Acc: 0.617999 loss: 0.011912\n",
      "[036/300] 11.70 sec(s) Train Acc: 0.840607 Loss: 0.003447 | Val Acc: 0.629702 loss: 0.012209\n",
      "[037/300] 10.28 sec(s) Train Acc: 0.841861 Loss: 0.003402 | Val Acc: 0.622736 loss: 0.012625\n",
      "[038/300] 10.57 sec(s) Train Acc: 0.845623 Loss: 0.003331 | Val Acc: 0.621343 loss: 0.012736\n",
      "[039/300] 10.32 sec(s) Train Acc: 0.852241 Loss: 0.003193 | Val Acc: 0.621900 loss: 0.013062\n",
      "[040/300] 10.31 sec(s) Train Acc: 0.861646 Loss: 0.002991 | Val Acc: 0.622458 loss: 0.014740\n",
      "[041/300] 10.28 sec(s) Train Acc: 0.860636 Loss: 0.003003 | Val Acc: 0.610476 loss: 0.013994\n",
      "[042/300] 10.72 sec(s) Train Acc: 0.867324 Loss: 0.002874 | Val Acc: 0.614099 loss: 0.014064\n",
      "[043/300] 10.28 sec(s) Train Acc: 0.869797 Loss: 0.002857 | Val Acc: 0.611034 loss: 0.013692\n",
      "[044/300] 10.34 sec(s) Train Acc: 0.873141 Loss: 0.002758 | Val Acc: 0.626358 loss: 0.014307\n",
      "[045/300] 10.35 sec(s) Train Acc: 0.874186 Loss: 0.002749 | Val Acc: 0.626080 loss: 0.013327\n",
      "[046/300] 10.35 sec(s) Train Acc: 0.878435 Loss: 0.002631 | Val Acc: 0.623293 loss: 0.014535\n",
      "[047/300] 10.56 sec(s) Train Acc: 0.883207 Loss: 0.002617 | Val Acc: 0.619950 loss: 0.013914\n",
      "[048/300] 10.32 sec(s) Train Acc: 0.886691 Loss: 0.002518 | Val Acc: 0.626080 loss: 0.014438\n",
      "[049/300] 10.32 sec(s) Train Acc: 0.890243 Loss: 0.002399 | Val Acc: 0.614935 loss: 0.014898\n",
      "[050/300] 11.45 sec(s) Train Acc: 0.893483 Loss: 0.002417 | Val Acc: 0.619393 loss: 0.016739\n",
      "[051/300] 10.33 sec(s) Train Acc: 0.893936 Loss: 0.002384 | Val Acc: 0.615492 loss: 0.016686\n",
      "[052/300] 10.35 sec(s) Train Acc: 0.889233 Loss: 0.002434 | Val Acc: 0.616049 loss: 0.018115\n",
      "[053/300] 10.34 sec(s) Train Acc: 0.900589 Loss: 0.002245 | Val Acc: 0.600446 loss: 0.016545\n",
      "[054/300] 10.35 sec(s) Train Acc: 0.902017 Loss: 0.002177 | Val Acc: 0.611312 loss: 0.018734\n",
      "[055/300] 10.36 sec(s) Train Acc: 0.900554 Loss: 0.002237 | Val Acc: 0.616328 loss: 0.015973\n",
      "[056/300] 10.35 sec(s) Train Acc: 0.906197 Loss: 0.002123 | Val Acc: 0.624687 loss: 0.016893\n",
      "[057/300] 10.34 sec(s) Train Acc: 0.909993 Loss: 0.002017 | Val Acc: 0.625244 loss: 0.017040\n",
      "[058/300] 10.33 sec(s) Train Acc: 0.907451 Loss: 0.002058 | Val Acc: 0.619114 loss: 0.015060\n",
      "[059/300] 10.34 sec(s) Train Acc: 0.912780 Loss: 0.001983 | Val Acc: 0.624965 loss: 0.017453\n",
      "[060/300] 10.33 sec(s) Train Acc: 0.907207 Loss: 0.002061 | Val Acc: 0.610198 loss: 0.018455\n",
      "[061/300] 10.34 sec(s) Train Acc: 0.913790 Loss: 0.001950 | Val Acc: 0.623015 loss: 0.018506\n",
      "[062/300] 10.44 sec(s) Train Acc: 0.913964 Loss: 0.001930 | Val Acc: 0.623015 loss: 0.018029\n",
      "[063/300] 10.37 sec(s) Train Acc: 0.915776 Loss: 0.001884 | Val Acc: 0.617721 loss: 0.016683\n",
      "[064/300] 10.35 sec(s) Train Acc: 0.919468 Loss: 0.001834 | Val Acc: 0.609919 loss: 0.018174\n",
      "[065/300] 10.36 sec(s) Train Acc: 0.919363 Loss: 0.001805 | Val Acc: 0.628866 loss: 0.018165\n",
      "[066/300] 11.41 sec(s) Train Acc: 0.918771 Loss: 0.001867 | Val Acc: 0.621343 loss: 0.017557\n",
      "[067/300] 11.68 sec(s) Train Acc: 0.922568 Loss: 0.001780 | Val Acc: 0.616049 loss: 0.017362\n",
      "[068/300] 11.67 sec(s) Train Acc: 0.924135 Loss: 0.001769 | Val Acc: 0.613263 loss: 0.017853\n",
      "[069/300] 10.28 sec(s) Train Acc: 0.922568 Loss: 0.001762 | Val Acc: 0.623572 loss: 0.018802\n",
      "[070/300] 10.31 sec(s) Train Acc: 0.926295 Loss: 0.001691 | Val Acc: 0.615213 loss: 0.018257\n",
      "[071/300] 10.34 sec(s) Train Acc: 0.925354 Loss: 0.001741 | Val Acc: 0.615770 loss: 0.018622\n",
      "[072/300] 10.24 sec(s) Train Acc: 0.930475 Loss: 0.001626 | Val Acc: 0.619671 loss: 0.019538\n",
      "[073/300] 11.40 sec(s) Train Acc: 0.929883 Loss: 0.001624 | Val Acc: 0.624965 loss: 0.018175\n",
      "[074/300] 10.37 sec(s) Train Acc: 0.932530 Loss: 0.001545 | Val Acc: 0.618835 loss: 0.018913\n",
      "[075/300] 10.34 sec(s) Train Acc: 0.929081 Loss: 0.001627 | Val Acc: 0.603789 loss: 0.017934\n",
      "[076/300] 10.38 sec(s) Train Acc: 0.931694 Loss: 0.001566 | Val Acc: 0.607690 loss: 0.018657\n",
      "[077/300] 10.30 sec(s) Train Acc: 0.933018 Loss: 0.001540 | Val Acc: 0.614935 loss: 0.018960\n",
      "[078/300] 10.35 sec(s) Train Acc: 0.933645 Loss: 0.001518 | Val Acc: 0.617721 loss: 0.018813\n",
      "[079/300] 10.34 sec(s) Train Acc: 0.933853 Loss: 0.001545 | Val Acc: 0.622736 loss: 0.018517\n",
      "[080/300] 10.29 sec(s) Train Acc: 0.935700 Loss: 0.001498 | Val Acc: 0.602118 loss: 0.017920\n",
      "[081/300] 10.35 sec(s) Train Acc: 0.938486 Loss: 0.001446 | Val Acc: 0.625801 loss: 0.020198\n",
      "[082/300] 10.38 sec(s) Train Acc: 0.937755 Loss: 0.001489 | Val Acc: 0.621064 loss: 0.020724\n",
      "[083/300] 10.35 sec(s) Train Acc: 0.938451 Loss: 0.001437 | Val Acc: 0.624129 loss: 0.020878\n",
      "[084/300] 10.37 sec(s) Train Acc: 0.938765 Loss: 0.001455 | Val Acc: 0.621064 loss: 0.019951\n",
      "[085/300] 10.35 sec(s) Train Acc: 0.937058 Loss: 0.001452 | Val Acc: 0.615492 loss: 0.018236\n",
      "[086/300] 10.36 sec(s) Train Acc: 0.936745 Loss: 0.001452 | Val Acc: 0.625522 loss: 0.019844\n",
      "[087/300] 10.38 sec(s) Train Acc: 0.940924 Loss: 0.001373 | Val Acc: 0.627194 loss: 0.018757\n",
      "[088/300] 10.37 sec(s) Train Acc: 0.943850 Loss: 0.001318 | Val Acc: 0.632210 loss: 0.019615\n",
      "[089/300] 10.38 sec(s) Train Acc: 0.940437 Loss: 0.001386 | Val Acc: 0.633324 loss: 0.020426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[090/300] 10.37 sec(s) Train Acc: 0.941412 Loss: 0.001364 | Val Acc: 0.612705 loss: 0.020264\n",
      "[091/300] 11.53 sec(s) Train Acc: 0.944547 Loss: 0.001277 | Val Acc: 0.624687 loss: 0.020585\n",
      "[092/300] 10.32 sec(s) Train Acc: 0.944547 Loss: 0.001287 | Val Acc: 0.610198 loss: 0.019899\n",
      "[093/300] 10.32 sec(s) Train Acc: 0.945731 Loss: 0.001268 | Val Acc: 0.625522 loss: 0.022411\n",
      "[094/300] 10.33 sec(s) Train Acc: 0.942527 Loss: 0.001354 | Val Acc: 0.622179 loss: 0.020003\n",
      "[095/300] 10.33 sec(s) Train Acc: 0.946010 Loss: 0.001275 | Val Acc: 0.619671 loss: 0.020668\n",
      "[096/300] 10.39 sec(s) Train Acc: 0.944965 Loss: 0.001281 | Val Acc: 0.616049 loss: 0.021572\n",
      "[097/300] 10.33 sec(s) Train Acc: 0.946428 Loss: 0.001259 | Val Acc: 0.619950 loss: 0.019856\n",
      "[098/300] 11.55 sec(s) Train Acc: 0.947055 Loss: 0.001257 | Val Acc: 0.613820 loss: 0.020347\n",
      "[099/300] 11.54 sec(s) Train Acc: 0.945766 Loss: 0.001240 | Val Acc: 0.619671 loss: 0.021763\n",
      "[100/300] 10.25 sec(s) Train Acc: 0.951200 Loss: 0.001168 | Val Acc: 0.611312 loss: 0.021775\n",
      "[101/300] 10.26 sec(s) Train Acc: 0.946672 Loss: 0.001254 | Val Acc: 0.609919 loss: 0.020144\n",
      "[102/300] 10.25 sec(s) Train Acc: 0.946532 Loss: 0.001260 | Val Acc: 0.621064 loss: 0.020116\n",
      "[103/300] 10.24 sec(s) Train Acc: 0.949075 Loss: 0.001189 | Val Acc: 0.617164 loss: 0.021631\n",
      "[104/300] 10.25 sec(s) Train Acc: 0.946985 Loss: 0.001227 | Val Acc: 0.625244 loss: 0.021639\n",
      "[105/300] 10.25 sec(s) Train Acc: 0.950608 Loss: 0.001122 | Val Acc: 0.606018 loss: 0.021937\n",
      "[106/300] 10.26 sec(s) Train Acc: 0.947229 Loss: 0.001239 | Val Acc: 0.615770 loss: 0.020018\n",
      "[107/300] 10.28 sec(s) Train Acc: 0.950538 Loss: 0.001185 | Val Acc: 0.623851 loss: 0.021928\n",
      "[108/300] 11.33 sec(s) Train Acc: 0.952140 Loss: 0.001150 | Val Acc: 0.628587 loss: 0.020850\n",
      "[109/300] 11.62 sec(s) Train Acc: 0.953360 Loss: 0.001112 | Val Acc: 0.622179 loss: 0.022666\n",
      "[110/300] 10.35 sec(s) Train Acc: 0.952245 Loss: 0.001131 | Val Acc: 0.618557 loss: 0.022408\n",
      "[111/300] 10.32 sec(s) Train Acc: 0.952837 Loss: 0.001119 | Val Acc: 0.616885 loss: 0.022190\n",
      "[112/300] 10.32 sec(s) Train Acc: 0.953360 Loss: 0.001100 | Val Acc: 0.620786 loss: 0.021581\n",
      "[113/300] 10.34 sec(s) Train Acc: 0.956076 Loss: 0.001035 | Val Acc: 0.630816 loss: 0.022221\n",
      "[114/300] 10.32 sec(s) Train Acc: 0.953987 Loss: 0.001066 | Val Acc: 0.627751 loss: 0.021588\n",
      "[115/300] 10.75 sec(s) Train Acc: 0.954683 Loss: 0.001069 | Val Acc: 0.626637 loss: 0.023994\n",
      "[116/300] 11.44 sec(s) Train Acc: 0.951688 Loss: 0.001140 | Val Acc: 0.626637 loss: 0.021956\n",
      "[117/300] 10.94 sec(s) Train Acc: 0.955206 Loss: 0.001055 | Val Acc: 0.616606 loss: 0.024152\n",
      "[118/300] 10.30 sec(s) Train Acc: 0.958515 Loss: 0.001014 | Val Acc: 0.625801 loss: 0.022866\n",
      "[119/300] 10.35 sec(s) Train Acc: 0.957052 Loss: 0.001017 | Val Acc: 0.623851 loss: 0.021779\n",
      "[120/300] 10.34 sec(s) Train Acc: 0.957574 Loss: 0.001040 | Val Acc: 0.624129 loss: 0.023943\n",
      "[121/300] 10.34 sec(s) Train Acc: 0.954405 Loss: 0.001078 | Val Acc: 0.609641 loss: 0.022003\n",
      "[122/300] 10.34 sec(s) Train Acc: 0.956599 Loss: 0.001045 | Val Acc: 0.622736 loss: 0.021359\n",
      "[123/300] 10.34 sec(s) Train Acc: 0.956285 Loss: 0.001050 | Val Acc: 0.608526 loss: 0.025603\n",
      "[124/300] 10.34 sec(s) Train Acc: 0.954230 Loss: 0.001086 | Val Acc: 0.619671 loss: 0.022460\n",
      "[125/300] 11.13 sec(s) Train Acc: 0.956738 Loss: 0.001025 | Val Acc: 0.619950 loss: 0.022030\n",
      "[126/300] 10.34 sec(s) Train Acc: 0.955310 Loss: 0.001028 | Val Acc: 0.627473 loss: 0.022878\n",
      "[127/300] 10.38 sec(s) Train Acc: 0.957853 Loss: 0.000987 | Val Acc: 0.624687 loss: 0.023344\n",
      "[128/300] 10.33 sec(s) Train Acc: 0.958132 Loss: 0.001014 | Val Acc: 0.619393 loss: 0.021320\n",
      "[129/300] 10.31 sec(s) Train Acc: 0.960117 Loss: 0.000949 | Val Acc: 0.619671 loss: 0.021174\n",
      "[130/300] 10.32 sec(s) Train Acc: 0.962207 Loss: 0.000905 | Val Acc: 0.626916 loss: 0.022790\n",
      "[131/300] 10.33 sec(s) Train Acc: 0.960013 Loss: 0.000940 | Val Acc: 0.620228 loss: 0.023381\n",
      "[132/300] 11.46 sec(s) Train Acc: 0.960814 Loss: 0.000948 | Val Acc: 0.621064 loss: 0.020873\n",
      "[133/300] 10.25 sec(s) Train Acc: 0.960535 Loss: 0.000948 | Val Acc: 0.625244 loss: 0.023477\n",
      "[134/300] 10.24 sec(s) Train Acc: 0.959525 Loss: 0.000963 | Val Acc: 0.614935 loss: 0.022438\n",
      "[135/300] 10.23 sec(s) Train Acc: 0.958968 Loss: 0.000973 | Val Acc: 0.610755 loss: 0.021559\n",
      "[136/300] 10.25 sec(s) Train Acc: 0.957714 Loss: 0.000987 | Val Acc: 0.611034 loss: 0.023061\n",
      "[137/300] 10.23 sec(s) Train Acc: 0.960117 Loss: 0.000970 | Val Acc: 0.625522 loss: 0.021250\n",
      "[138/300] 10.23 sec(s) Train Acc: 0.959002 Loss: 0.000958 | Val Acc: 0.618278 loss: 0.022664\n",
      "[139/300] 10.23 sec(s) Train Acc: 0.962555 Loss: 0.000883 | Val Acc: 0.614935 loss: 0.024632\n",
      "[140/300] 10.23 sec(s) Train Acc: 0.960918 Loss: 0.000935 | Val Acc: 0.619393 loss: 0.022497\n",
      "[141/300] 11.41 sec(s) Train Acc: 0.961475 Loss: 0.000925 | Val Acc: 0.619393 loss: 0.022552\n",
      "[142/300] 11.62 sec(s) Train Acc: 0.960814 Loss: 0.000968 | Val Acc: 0.620228 loss: 0.023226\n",
      "[143/300] 10.27 sec(s) Train Acc: 0.962137 Loss: 0.000892 | Val Acc: 0.614377 loss: 0.023236\n",
      "[144/300] 10.27 sec(s) Train Acc: 0.960779 Loss: 0.000909 | Val Acc: 0.622458 loss: 0.023869\n",
      "[145/300] 10.26 sec(s) Train Acc: 0.962520 Loss: 0.000881 | Val Acc: 0.617999 loss: 0.023624\n",
      "[146/300] 10.26 sec(s) Train Acc: 0.963322 Loss: 0.000870 | Val Acc: 0.613263 loss: 0.024434\n",
      "[147/300] 10.29 sec(s) Train Acc: 0.962799 Loss: 0.000888 | Val Acc: 0.613820 loss: 0.024380\n",
      "[148/300] 11.22 sec(s) Train Acc: 0.963287 Loss: 0.000867 | Val Acc: 0.619950 loss: 0.024562\n",
      "[149/300] 10.24 sec(s) Train Acc: 0.965377 Loss: 0.000831 | Val Acc: 0.618557 loss: 0.022890\n",
      "[150/300] 10.24 sec(s) Train Acc: 0.962764 Loss: 0.000926 | Val Acc: 0.604904 loss: 0.023093\n",
      "[151/300] 10.23 sec(s) Train Acc: 0.964297 Loss: 0.000866 | Val Acc: 0.620786 loss: 0.023581\n",
      "[152/300] 10.35 sec(s) Train Acc: 0.965377 Loss: 0.000822 | Val Acc: 0.623293 loss: 0.023967\n",
      "[153/300] 10.34 sec(s) Train Acc: 0.963879 Loss: 0.000862 | Val Acc: 0.622736 loss: 0.023216\n",
      "[154/300] 11.22 sec(s) Train Acc: 0.961859 Loss: 0.000906 | Val Acc: 0.619393 loss: 0.022634\n",
      "[155/300] 10.37 sec(s) Train Acc: 0.961371 Loss: 0.000919 | Val Acc: 0.612148 loss: 0.021496\n",
      "[156/300] 10.33 sec(s) Train Acc: 0.963565 Loss: 0.000851 | Val Acc: 0.618278 loss: 0.024532\n",
      "[157/300] 10.31 sec(s) Train Acc: 0.965412 Loss: 0.000844 | Val Acc: 0.620507 loss: 0.022761\n",
      "[158/300] 10.31 sec(s) Train Acc: 0.965864 Loss: 0.000832 | Val Acc: 0.617442 loss: 0.024175\n",
      "[159/300] 10.35 sec(s) Train Acc: 0.963705 Loss: 0.000854 | Val Acc: 0.629702 loss: 0.022425\n",
      "[160/300] 10.35 sec(s) Train Acc: 0.964158 Loss: 0.000845 | Val Acc: 0.622179 loss: 0.021901\n",
      "[161/300] 10.36 sec(s) Train Acc: 0.965795 Loss: 0.000798 | Val Acc: 0.622736 loss: 0.022781\n",
      "[162/300] 10.36 sec(s) Train Acc: 0.966979 Loss: 0.000804 | Val Acc: 0.621900 loss: 0.023149\n",
      "[163/300] 10.35 sec(s) Train Acc: 0.965307 Loss: 0.000819 | Val Acc: 0.628866 loss: 0.024883\n",
      "[164/300] 10.36 sec(s) Train Acc: 0.965133 Loss: 0.000816 | Val Acc: 0.623851 loss: 0.024795\n",
      "[165/300] 10.39 sec(s) Train Acc: 0.963914 Loss: 0.000864 | Val Acc: 0.621343 loss: 0.025833\n",
      "[166/300] 11.73 sec(s) Train Acc: 0.966073 Loss: 0.000833 | Val Acc: 0.612148 loss: 0.022746\n",
      "[167/300] 11.60 sec(s) Train Acc: 0.967292 Loss: 0.000794 | Val Acc: 0.625801 loss: 0.025095\n",
      "[168/300] 10.28 sec(s) Train Acc: 0.967432 Loss: 0.000796 | Val Acc: 0.611591 loss: 0.022364\n",
      "[169/300] 10.25 sec(s) Train Acc: 0.967536 Loss: 0.000797 | Val Acc: 0.615492 loss: 0.024284\n",
      "[170/300] 10.27 sec(s) Train Acc: 0.968337 Loss: 0.000767 | Val Acc: 0.628309 loss: 0.024497\n",
      "[171/300] 11.41 sec(s) Train Acc: 0.966770 Loss: 0.000780 | Val Acc: 0.615492 loss: 0.022992\n",
      "[172/300] 10.25 sec(s) Train Acc: 0.968546 Loss: 0.000753 | Val Acc: 0.612984 loss: 0.022681\n",
      "[173/300] 10.25 sec(s) Train Acc: 0.966770 Loss: 0.000782 | Val Acc: 0.619950 loss: 0.023655\n",
      "[174/300] 10.25 sec(s) Train Acc: 0.967850 Loss: 0.000781 | Val Acc: 0.626080 loss: 0.024047\n",
      "[175/300] 10.26 sec(s) Train Acc: 0.967188 Loss: 0.000809 | Val Acc: 0.621622 loss: 0.023354\n",
      "[176/300] 10.32 sec(s) Train Acc: 0.967780 Loss: 0.000750 | Val Acc: 0.623293 loss: 0.025613\n",
      "[177/300] 10.35 sec(s) Train Acc: 0.969417 Loss: 0.000748 | Val Acc: 0.603789 loss: 0.025103\n",
      "[178/300] 10.34 sec(s) Train Acc: 0.968512 Loss: 0.000767 | Val Acc: 0.610198 loss: 0.023540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[179/300] 10.27 sec(s) Train Acc: 0.968024 Loss: 0.000761 | Val Acc: 0.623572 loss: 0.023480\n",
      "[180/300] 10.27 sec(s) Train Acc: 0.968163 Loss: 0.000755 | Val Acc: 0.622736 loss: 0.022417\n",
      "[181/300] 10.28 sec(s) Train Acc: 0.971124 Loss: 0.000696 | Val Acc: 0.627473 loss: 0.025452\n",
      "[182/300] 10.29 sec(s) Train Acc: 0.970636 Loss: 0.000720 | Val Acc: 0.626916 loss: 0.026923\n",
      "[183/300] 11.30 sec(s) Train Acc: 0.968721 Loss: 0.000761 | Val Acc: 0.621064 loss: 0.025070\n",
      "[184/300] 10.35 sec(s) Train Acc: 0.968372 Loss: 0.000746 | Val Acc: 0.620228 loss: 0.024484\n",
      "[185/300] 10.35 sec(s) Train Acc: 0.970845 Loss: 0.000703 | Val Acc: 0.625801 loss: 0.024766\n",
      "[186/300] 10.35 sec(s) Train Acc: 0.969626 Loss: 0.000730 | Val Acc: 0.615213 loss: 0.023614\n",
      "[187/300] 11.49 sec(s) Train Acc: 0.969348 Loss: 0.000749 | Val Acc: 0.625244 loss: 0.023003\n",
      "[188/300] 10.36 sec(s) Train Acc: 0.967327 Loss: 0.000768 | Val Acc: 0.610755 loss: 0.023887\n",
      "[189/300] 11.74 sec(s) Train Acc: 0.969800 Loss: 0.000729 | Val Acc: 0.617999 loss: 0.024349\n",
      "[190/300] 10.33 sec(s) Train Acc: 0.969800 Loss: 0.000724 | Val Acc: 0.610755 loss: 0.023219\n",
      "[191/300] 10.36 sec(s) Train Acc: 0.969557 Loss: 0.000715 | Val Acc: 0.614656 loss: 0.023681\n",
      "[192/300] 10.32 sec(s) Train Acc: 0.969452 Loss: 0.000729 | Val Acc: 0.614656 loss: 0.025749\n",
      "[193/300] 10.37 sec(s) Train Acc: 0.972274 Loss: 0.000661 | Val Acc: 0.624408 loss: 0.024242\n",
      "[194/300] 10.38 sec(s) Train Acc: 0.972030 Loss: 0.000668 | Val Acc: 0.619671 loss: 0.024254\n",
      "[195/300] 11.59 sec(s) Train Acc: 0.971890 Loss: 0.000683 | Val Acc: 0.615213 loss: 0.024282\n",
      "[196/300] 11.80 sec(s) Train Acc: 0.969069 Loss: 0.000733 | Val Acc: 0.615213 loss: 0.025285\n",
      "[197/300] 10.37 sec(s) Train Acc: 0.969557 Loss: 0.000758 | Val Acc: 0.619393 loss: 0.023549\n",
      "[198/300] 10.36 sec(s) Train Acc: 0.972448 Loss: 0.000683 | Val Acc: 0.624129 loss: 0.023809\n",
      "[199/300] 10.35 sec(s) Train Acc: 0.971298 Loss: 0.000695 | Val Acc: 0.625244 loss: 0.025158\n",
      "[200/300] 10.32 sec(s) Train Acc: 0.972413 Loss: 0.000656 | Val Acc: 0.616606 loss: 0.023631\n",
      "[201/300] 10.27 sec(s) Train Acc: 0.973597 Loss: 0.000645 | Val Acc: 0.615770 loss: 0.025151\n",
      "[202/300] 10.25 sec(s) Train Acc: 0.973458 Loss: 0.000644 | Val Acc: 0.629423 loss: 0.024707\n",
      "[203/300] 10.26 sec(s) Train Acc: 0.970950 Loss: 0.000701 | Val Acc: 0.632767 loss: 0.025366\n",
      "[204/300] 10.27 sec(s) Train Acc: 0.970114 Loss: 0.000699 | Val Acc: 0.624408 loss: 0.023572\n",
      "[205/300] 10.27 sec(s) Train Acc: 0.970880 Loss: 0.000707 | Val Acc: 0.621622 loss: 0.026238\n",
      "[206/300] 10.26 sec(s) Train Acc: 0.971124 Loss: 0.000697 | Val Acc: 0.621622 loss: 0.024623\n",
      "[207/300] 10.27 sec(s) Train Acc: 0.970393 Loss: 0.000698 | Val Acc: 0.625522 loss: 0.024471\n",
      "[208/300] 11.58 sec(s) Train Acc: 0.971472 Loss: 0.000691 | Val Acc: 0.628866 loss: 0.025200\n",
      "[209/300] 10.31 sec(s) Train Acc: 0.972935 Loss: 0.000679 | Val Acc: 0.623293 loss: 0.023365\n",
      "[210/300] 10.29 sec(s) Train Acc: 0.973040 Loss: 0.000636 | Val Acc: 0.633045 loss: 0.028073\n",
      "[211/300] 10.27 sec(s) Train Acc: 0.972552 Loss: 0.000695 | Val Acc: 0.634717 loss: 0.025092\n",
      "[212/300] 11.36 sec(s) Train Acc: 0.971751 Loss: 0.000677 | Val Acc: 0.620228 loss: 0.024365\n",
      "[213/300] 10.28 sec(s) Train Acc: 0.973911 Loss: 0.000630 | Val Acc: 0.623293 loss: 0.026731\n",
      "[214/300] 10.27 sec(s) Train Acc: 0.972308 Loss: 0.000658 | Val Acc: 0.620786 loss: 0.026797\n",
      "[215/300] 10.27 sec(s) Train Acc: 0.972935 Loss: 0.000671 | Val Acc: 0.628587 loss: 0.027384\n",
      "[216/300] 10.30 sec(s) Train Acc: 0.972482 Loss: 0.000660 | Val Acc: 0.618835 loss: 0.025492\n",
      "[217/300] 10.27 sec(s) Train Acc: 0.972413 Loss: 0.000671 | Val Acc: 0.613820 loss: 0.023229\n",
      "[218/300] 10.31 sec(s) Train Acc: 0.975408 Loss: 0.000598 | Val Acc: 0.625244 loss: 0.028195\n",
      "[219/300] 10.28 sec(s) Train Acc: 0.972657 Loss: 0.000669 | Val Acc: 0.608526 loss: 0.023397\n",
      "[220/300] 10.24 sec(s) Train Acc: 0.974921 Loss: 0.000616 | Val Acc: 0.620507 loss: 0.024006\n",
      "[221/300] 10.25 sec(s) Train Acc: 0.973597 Loss: 0.000651 | Val Acc: 0.623851 loss: 0.026991\n",
      "[222/300] 11.20 sec(s) Train Acc: 0.974781 Loss: 0.000633 | Val Acc: 0.629145 loss: 0.027072\n",
      "[223/300] 10.23 sec(s) Train Acc: 0.974015 Loss: 0.000615 | Val Acc: 0.619671 loss: 0.024091\n",
      "[224/300] 10.24 sec(s) Train Acc: 0.975025 Loss: 0.000620 | Val Acc: 0.626916 loss: 0.027091\n",
      "[225/300] 10.23 sec(s) Train Acc: 0.973179 Loss: 0.000658 | Val Acc: 0.623293 loss: 0.024032\n",
      "[226/300] 10.86 sec(s) Train Acc: 0.973388 Loss: 0.000637 | Val Acc: 0.625522 loss: 0.024966\n",
      "[227/300] 11.73 sec(s) Train Acc: 0.974189 Loss: 0.000628 | Val Acc: 0.618557 loss: 0.025406\n",
      "[228/300] 11.65 sec(s) Train Acc: 0.974956 Loss: 0.000618 | Val Acc: 0.627473 loss: 0.027135\n",
      "[229/300] 10.28 sec(s) Train Acc: 0.975861 Loss: 0.000596 | Val Acc: 0.616049 loss: 0.025710\n",
      "[230/300] 10.27 sec(s) Train Acc: 0.975374 Loss: 0.000606 | Val Acc: 0.624687 loss: 0.027319\n",
      "[231/300] 10.26 sec(s) Train Acc: 0.974120 Loss: 0.000634 | Val Acc: 0.626358 loss: 0.026316\n",
      "[232/300] 10.32 sec(s) Train Acc: 0.973493 Loss: 0.000622 | Val Acc: 0.625244 loss: 0.027227\n",
      "[233/300] 10.37 sec(s) Train Acc: 0.975966 Loss: 0.000607 | Val Acc: 0.631652 loss: 0.027145\n",
      "[234/300] 10.36 sec(s) Train Acc: 0.973527 Loss: 0.000627 | Val Acc: 0.629423 loss: 0.025552\n",
      "[235/300] 10.34 sec(s) Train Acc: 0.976035 Loss: 0.000571 | Val Acc: 0.623572 loss: 0.027640\n",
      "[236/300] 10.27 sec(s) Train Acc: 0.975199 Loss: 0.000616 | Val Acc: 0.623015 loss: 0.024770\n",
      "[237/300] 10.25 sec(s) Train Acc: 0.976314 Loss: 0.000581 | Val Acc: 0.626637 loss: 0.025582\n",
      "[238/300] 10.26 sec(s) Train Acc: 0.974224 Loss: 0.000636 | Val Acc: 0.619950 loss: 0.025353\n",
      "[239/300] 10.32 sec(s) Train Acc: 0.975443 Loss: 0.000593 | Val Acc: 0.625522 loss: 0.024395\n",
      "[240/300] 10.31 sec(s) Train Acc: 0.976802 Loss: 0.000577 | Val Acc: 0.616049 loss: 0.025562\n",
      "[241/300] 10.35 sec(s) Train Acc: 0.976976 Loss: 0.000572 | Val Acc: 0.623293 loss: 0.025726\n",
      "[242/300] 10.41 sec(s) Train Acc: 0.976035 Loss: 0.000590 | Val Acc: 0.620228 loss: 0.026360\n",
      "[243/300] 11.47 sec(s) Train Acc: 0.976384 Loss: 0.000560 | Val Acc: 0.622458 loss: 0.027508\n",
      "[244/300] 11.67 sec(s) Train Acc: 0.975861 Loss: 0.000600 | Val Acc: 0.628030 loss: 0.027442\n",
      "[245/300] 10.33 sec(s) Train Acc: 0.975826 Loss: 0.000587 | Val Acc: 0.626637 loss: 0.027999\n",
      "[246/300] 10.33 sec(s) Train Acc: 0.974747 Loss: 0.000621 | Val Acc: 0.625801 loss: 0.027014\n",
      "[247/300] 10.33 sec(s) Train Acc: 0.976314 Loss: 0.000568 | Val Acc: 0.620786 loss: 0.028178\n",
      "[248/300] 10.31 sec(s) Train Acc: 0.977150 Loss: 0.000562 | Val Acc: 0.616328 loss: 0.026198\n",
      "[249/300] 10.26 sec(s) Train Acc: 0.977777 Loss: 0.000547 | Val Acc: 0.626637 loss: 0.023966\n",
      "[250/300] 10.27 sec(s) Train Acc: 0.977359 Loss: 0.000570 | Val Acc: 0.624687 loss: 0.026793\n",
      "[251/300] 10.27 sec(s) Train Acc: 0.978543 Loss: 0.000545 | Val Acc: 0.620786 loss: 0.025795\n",
      "[252/300] 10.87 sec(s) Train Acc: 0.975583 Loss: 0.000581 | Val Acc: 0.614935 loss: 0.027820\n",
      "[253/300] 11.60 sec(s) Train Acc: 0.979902 Loss: 0.000503 | Val Acc: 0.628587 loss: 0.027909\n",
      "[254/300] 10.97 sec(s) Train Acc: 0.975722 Loss: 0.000619 | Val Acc: 0.621900 loss: 0.023955\n",
      "[255/300] 11.67 sec(s) Train Acc: 0.977464 Loss: 0.000550 | Val Acc: 0.612984 loss: 0.025865\n",
      "[256/300] 11.62 sec(s) Train Acc: 0.977986 Loss: 0.000534 | Val Acc: 0.621622 loss: 0.025455\n",
      "[257/300] 10.34 sec(s) Train Acc: 0.976314 Loss: 0.000569 | Val Acc: 0.608805 loss: 0.030490\n",
      "[258/300] 10.32 sec(s) Train Acc: 0.976906 Loss: 0.000596 | Val Acc: 0.615213 loss: 0.023835\n",
      "[259/300] 11.36 sec(s) Train Acc: 0.978892 Loss: 0.000504 | Val Acc: 0.620507 loss: 0.030465\n",
      "[260/300] 10.34 sec(s) Train Acc: 0.977986 Loss: 0.000569 | Val Acc: 0.622736 loss: 0.025882\n",
      "[261/300] 10.35 sec(s) Train Acc: 0.976662 Loss: 0.000587 | Val Acc: 0.623572 loss: 0.027238\n",
      "[262/300] 10.32 sec(s) Train Acc: 0.976210 Loss: 0.000596 | Val Acc: 0.628030 loss: 0.027946\n",
      "[263/300] 10.31 sec(s) Train Acc: 0.978090 Loss: 0.000546 | Val Acc: 0.610476 loss: 0.028066\n",
      "[264/300] 10.33 sec(s) Train Acc: 0.976140 Loss: 0.000552 | Val Acc: 0.619393 loss: 0.027244\n",
      "[265/300] 10.33 sec(s) Train Acc: 0.976628 Loss: 0.000570 | Val Acc: 0.627751 loss: 0.027387\n",
      "[266/300] 10.34 sec(s) Train Acc: 0.979310 Loss: 0.000504 | Val Acc: 0.624687 loss: 0.027034\n",
      "[267/300] 10.35 sec(s) Train Acc: 0.978056 Loss: 0.000543 | Val Acc: 0.624965 loss: 0.028279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[268/300] 10.34 sec(s) Train Acc: 0.979693 Loss: 0.000489 | Val Acc: 0.631095 loss: 0.027649\n",
      "[269/300] 10.37 sec(s) Train Acc: 0.979449 Loss: 0.000492 | Val Acc: 0.620786 loss: 0.031775\n",
      "[270/300] 11.53 sec(s) Train Acc: 0.977707 Loss: 0.000541 | Val Acc: 0.623572 loss: 0.026113\n",
      "[271/300] 10.34 sec(s) Train Acc: 0.979728 Loss: 0.000497 | Val Acc: 0.610755 loss: 0.028796\n",
      "[272/300] 10.32 sec(s) Train Acc: 0.978474 Loss: 0.000538 | Val Acc: 0.615213 loss: 0.028914\n",
      "[273/300] 10.32 sec(s) Train Acc: 0.979484 Loss: 0.000517 | Val Acc: 0.615492 loss: 0.027856\n",
      "[274/300] 10.87 sec(s) Train Acc: 0.979867 Loss: 0.000508 | Val Acc: 0.614099 loss: 0.029038\n",
      "[275/300] 10.30 sec(s) Train Acc: 0.978683 Loss: 0.000533 | Val Acc: 0.622458 loss: 0.028345\n",
      "[276/300] 10.33 sec(s) Train Acc: 0.977882 Loss: 0.000550 | Val Acc: 0.620786 loss: 0.026346\n",
      "[277/300] 10.31 sec(s) Train Acc: 0.979031 Loss: 0.000507 | Val Acc: 0.622736 loss: 0.027546\n",
      "[278/300] 10.33 sec(s) Train Acc: 0.978926 Loss: 0.000512 | Val Acc: 0.625522 loss: 0.025834\n",
      "[279/300] 11.50 sec(s) Train Acc: 0.978334 Loss: 0.000520 | Val Acc: 0.607690 loss: 0.027258\n",
      "[280/300] 10.35 sec(s) Train Acc: 0.978683 Loss: 0.000519 | Val Acc: 0.621900 loss: 0.025107\n",
      "[281/300] 10.34 sec(s) Train Acc: 0.979519 Loss: 0.000519 | Val Acc: 0.617721 loss: 0.027675\n",
      "[282/300] 10.37 sec(s) Train Acc: 0.978926 Loss: 0.000510 | Val Acc: 0.599889 loss: 0.024612\n",
      "[283/300] 11.01 sec(s) Train Acc: 0.980668 Loss: 0.000480 | Val Acc: 0.619393 loss: 0.027356\n",
      "[284/300] 10.33 sec(s) Train Acc: 0.977150 Loss: 0.000554 | Val Acc: 0.622736 loss: 0.027909\n",
      "[285/300] 10.33 sec(s) Train Acc: 0.979414 Loss: 0.000520 | Val Acc: 0.622458 loss: 0.027059\n",
      "[286/300] 10.33 sec(s) Train Acc: 0.980355 Loss: 0.000471 | Val Acc: 0.621064 loss: 0.028026\n",
      "[287/300] 10.32 sec(s) Train Acc: 0.978648 Loss: 0.000540 | Val Acc: 0.618835 loss: 0.025277\n",
      "[288/300] 10.32 sec(s) Train Acc: 0.981086 Loss: 0.000486 | Val Acc: 0.614656 loss: 0.026918\n",
      "[289/300] 10.33 sec(s) Train Acc: 0.977464 Loss: 0.000555 | Val Acc: 0.625244 loss: 0.027415\n",
      "[290/300] 10.34 sec(s) Train Acc: 0.980459 Loss: 0.000475 | Val Acc: 0.626916 loss: 0.029202\n",
      "[291/300] 10.34 sec(s) Train Acc: 0.980215 Loss: 0.000500 | Val Acc: 0.621622 loss: 0.029489\n",
      "[292/300] 10.34 sec(s) Train Acc: 0.982410 Loss: 0.000467 | Val Acc: 0.626916 loss: 0.026855\n",
      "[293/300] 10.31 sec(s) Train Acc: 0.980180 Loss: 0.000502 | Val Acc: 0.624408 loss: 0.028477\n",
      "[294/300] 10.25 sec(s) Train Acc: 0.980633 Loss: 0.000490 | Val Acc: 0.621622 loss: 0.028139\n",
      "[295/300] 10.25 sec(s) Train Acc: 0.980773 Loss: 0.000486 | Val Acc: 0.618557 loss: 0.027068\n",
      "[296/300] 10.28 sec(s) Train Acc: 0.982375 Loss: 0.000441 | Val Acc: 0.620228 loss: 0.026807\n",
      "[297/300] 10.25 sec(s) Train Acc: 0.979937 Loss: 0.000479 | Val Acc: 0.624129 loss: 0.026234\n",
      "[298/300] 10.32 sec(s) Train Acc: 0.980041 Loss: 0.000488 | Val Acc: 0.623015 loss: 0.026032\n",
      "[299/300] 10.35 sec(s) Train Acc: 0.980285 Loss: 0.000494 | Val Acc: 0.610755 loss: 0.025750\n",
      "[300/300] 10.36 sec(s) Train Acc: 0.980076 Loss: 0.000471 | Val Acc: 0.620507 loss: 0.028461\n"
     ]
    }
   ],
   "source": [
    "model_1 = Classifier_1()\n",
    "train_test(model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试一下，扩大图像。因为一开始图像的size有点小，图也比较模糊，可能特征没抓取好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Classifier_2(nn.Module):\n",
    "    def __init__(self):\n",
    "        # super继承父类\n",
    "        super(Classifier_2, self).__init__()\n",
    "        \n",
    "        # conv2d( in_channels, out_channels, kernel_size, stide, padding)\n",
    "        # input shape: [1, 48, 48]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 2, 1, 1), \n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 1, 1), \n",
    "            \n",
    "            nn.Conv2d(64, 128, 2, 1, 1), \n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0),\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, 1, 1), \n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0), \n",
    "            \n",
    "            nn.Conv2d(256, 256, 3, 1, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      \n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(9216, 512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 7)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      name   class_name        input_shape       output_shape        nb_params\n",
      "1    cnn.0       Conv2d    (-1, 1, 48, 48)   (-1, 64, 49, 49)      tensor(320)\n",
      "2    cnn.1  BatchNorm2d   (-1, 64, 49, 49)   (-1, 64, 49, 49)      tensor(128)\n",
      "3    cnn.2         ReLU   (-1, 64, 49, 49)   (-1, 64, 49, 49)                0\n",
      "4    cnn.3    MaxPool2d   (-1, 64, 49, 49)   (-1, 64, 50, 50)                0\n",
      "5    cnn.4       Conv2d   (-1, 64, 50, 50)  (-1, 128, 51, 51)    tensor(32896)\n",
      "6    cnn.5  BatchNorm2d  (-1, 128, 51, 51)  (-1, 128, 51, 51)      tensor(256)\n",
      "7    cnn.6         ReLU  (-1, 128, 51, 51)  (-1, 128, 51, 51)                0\n",
      "8    cnn.7    MaxPool2d  (-1, 128, 51, 51)  (-1, 128, 25, 25)                0\n",
      "9    cnn.8       Conv2d  (-1, 128, 25, 25)  (-1, 256, 25, 25)   tensor(295168)\n",
      "10   cnn.9  BatchNorm2d  (-1, 256, 25, 25)  (-1, 256, 25, 25)      tensor(512)\n",
      "11  cnn.10         ReLU  (-1, 256, 25, 25)  (-1, 256, 25, 25)                0\n",
      "12  cnn.11    MaxPool2d  (-1, 256, 25, 25)  (-1, 256, 12, 12)                0\n",
      "13  cnn.12       Conv2d  (-1, 256, 12, 12)  (-1, 256, 12, 12)   tensor(590080)\n",
      "14  cnn.13  BatchNorm2d  (-1, 256, 12, 12)  (-1, 256, 12, 12)      tensor(512)\n",
      "15  cnn.14         ReLU  (-1, 256, 12, 12)  (-1, 256, 12, 12)                0\n",
      "16  cnn.15    MaxPool2d  (-1, 256, 12, 12)    (-1, 256, 6, 6)                0\n",
      "17    fc.0       Linear         (-1, 9216)          (-1, 512)  tensor(4719104)\n",
      "18    fc.1      Dropout          (-1, 512)          (-1, 512)                0\n",
      "19    fc.2         ReLU          (-1, 512)          (-1, 512)                0\n",
      "20    fc.3       Linear          (-1, 512)          (-1, 256)   tensor(131328)\n",
      "21    fc.4      Dropout          (-1, 256)          (-1, 256)                0\n",
      "22    fc.5       Linear          (-1, 256)            (-1, 7)     tensor(1799)\n"
     ]
    }
   ],
   "source": [
    "model_2 = Classifier_2()\n",
    "show_summary(model_2, (1,48,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] 15.66 sec(s) Train Acc: 0.679717 Loss: 0.006690 | Val Acc: 0.605740 loss: 0.008994\n",
      "[002/030] 14.25 sec(s) Train Acc: 0.691421 Loss: 0.006434 | Val Acc: 0.598217 loss: 0.009222\n",
      "[003/030] 14.27 sec(s) Train Acc: 0.697900 Loss: 0.006361 | Val Acc: 0.612148 loss: 0.009737\n",
      "[004/030] 14.26 sec(s) Train Acc: 0.696367 Loss: 0.006386 | Val Acc: 0.613820 loss: 0.009218\n",
      "[005/030] 14.16 sec(s) Train Acc: 0.702045 Loss: 0.006217 | Val Acc: 0.616049 loss: 0.009458\n",
      "[006/030] 14.29 sec(s) Train Acc: 0.709534 Loss: 0.006086 | Val Acc: 0.614935 loss: 0.009945\n",
      "[007/030] 14.33 sec(s) Train Acc: 0.710265 Loss: 0.006052 | Val Acc: 0.617164 loss: 0.009822\n",
      "[008/030] 14.32 sec(s) Train Acc: 0.718451 Loss: 0.005932 | Val Acc: 0.606297 loss: 0.010312\n",
      "[009/030] 14.30 sec(s) Train Acc: 0.720088 Loss: 0.005839 | Val Acc: 0.614935 loss: 0.009941\n",
      "[010/030] 14.32 sec(s) Train Acc: 0.731060 Loss: 0.005677 | Val Acc: 0.615492 loss: 0.009861\n",
      "[011/030] 15.31 sec(s) Train Acc: 0.728657 Loss: 0.005714 | Val Acc: 0.598217 loss: 0.011017\n",
      "[012/030] 14.30 sec(s) Train Acc: 0.737295 Loss: 0.005550 | Val Acc: 0.599053 loss: 0.009627\n",
      "[013/030] 14.32 sec(s) Train Acc: 0.743530 Loss: 0.005476 | Val Acc: 0.611312 loss: 0.011429\n",
      "[014/030] 14.32 sec(s) Train Acc: 0.748267 Loss: 0.005346 | Val Acc: 0.615492 loss: 0.010896\n",
      "[015/030] 14.35 sec(s) Train Acc: 0.752517 Loss: 0.005218 | Val Acc: 0.599889 loss: 0.010754\n",
      "[016/030] 14.32 sec(s) Train Acc: 0.755443 Loss: 0.005231 | Val Acc: 0.617999 loss: 0.011476\n",
      "[017/030] 14.32 sec(s) Train Acc: 0.757846 Loss: 0.005139 | Val Acc: 0.616885 loss: 0.011246\n",
      "[018/030] 14.40 sec(s) Train Acc: 0.762513 Loss: 0.005085 | Val Acc: 0.608247 loss: 0.011598\n",
      "[019/030] 14.35 sec(s) Train Acc: 0.766101 Loss: 0.005041 | Val Acc: 0.607690 loss: 0.011497\n",
      "[020/030] 14.32 sec(s) Train Acc: 0.768539 Loss: 0.004970 | Val Acc: 0.617442 loss: 0.011992\n",
      "[021/030] 14.30 sec(s) Train Acc: 0.771326 Loss: 0.004907 | Val Acc: 0.610755 loss: 0.011756\n",
      "[022/030] 14.32 sec(s) Train Acc: 0.773729 Loss: 0.004870 | Val Acc: 0.592923 loss: 0.010883\n",
      "[023/030] 14.36 sec(s) Train Acc: 0.783761 Loss: 0.004715 | Val Acc: 0.608247 loss: 0.011610\n",
      "[024/030] 14.32 sec(s) Train Acc: 0.781602 Loss: 0.004704 | Val Acc: 0.610755 loss: 0.012013\n",
      "[025/030] 15.55 sec(s) Train Acc: 0.785050 Loss: 0.004652 | Val Acc: 0.614099 loss: 0.011271\n",
      "[026/030] 14.31 sec(s) Train Acc: 0.788429 Loss: 0.004562 | Val Acc: 0.607412 loss: 0.012495\n",
      "[027/030] 14.91 sec(s) Train Acc: 0.792783 Loss: 0.004513 | Val Acc: 0.612427 loss: 0.011634\n",
      "[028/030] 15.58 sec(s) Train Acc: 0.792504 Loss: 0.004465 | Val Acc: 0.607412 loss: 0.012542\n",
      "[029/030] 14.31 sec(s) Train Acc: 0.796893 Loss: 0.004421 | Val Acc: 0.605461 loss: 0.013799\n",
      "[030/030] 14.32 sec(s) Train Acc: 0.798495 Loss: 0.004386 | Val Acc: 0.611870 loss: 0.011360\n"
     ]
    }
   ],
   "source": [
    "train_test(model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 寻找到较好的模型参数后，合并train，val一起训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_x = np.concatenate((train_x, val_x), axis=0)\n",
    "train_val_y = np.concatenate((train_y, val_y), axis=0)\n",
    "train_val_set = MicroExpreDataset(train_val_x, train_val_y, train_transform)\n",
    "train_val_loader = DataLoader(train_val_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/030] 11.49 sec(s) Train Acc: 0.343767 Loss: 0.012870\n",
      "[002/030] 11.51 sec(s) Train Acc: 0.497090 Loss: 0.010235\n",
      "[003/030] 11.68 sec(s) Train Acc: 0.541891 Loss: 0.009361\n",
      "[004/030] 11.80 sec(s) Train Acc: 0.571274 Loss: 0.008760\n",
      "[005/030] 11.57 sec(s) Train Acc: 0.592390 Loss: 0.008395\n",
      "[006/030] 11.20 sec(s) Train Acc: 0.611400 Loss: 0.008077\n",
      "[007/030] 11.24 sec(s) Train Acc: 0.628584 Loss: 0.007733\n",
      "[008/030] 11.79 sec(s) Train Acc: 0.643260 Loss: 0.007445\n",
      "[009/030] 11.60 sec(s) Train Acc: 0.659669 Loss: 0.007096\n",
      "[010/030] 11.53 sec(s) Train Acc: 0.673695 Loss: 0.006792\n",
      "[011/030] 11.61 sec(s) Train Acc: 0.687999 Loss: 0.006540\n",
      "[012/030] 11.65 sec(s) Train Acc: 0.702551 Loss: 0.006268\n",
      "[013/030] 11.60 sec(s) Train Acc: 0.718001 Loss: 0.005917\n",
      "[014/030] 11.79 sec(s) Train Acc: 0.736795 Loss: 0.005612\n",
      "[015/030] 12.30 sec(s) Train Acc: 0.750480 Loss: 0.005300\n",
      "[016/030] 12.02 sec(s) Train Acc: 0.762029 Loss: 0.005118\n",
      "[017/030] 11.64 sec(s) Train Acc: 0.776828 Loss: 0.004728\n",
      "[018/030] 11.67 sec(s) Train Acc: 0.789089 Loss: 0.004534\n",
      "[019/030] 11.62 sec(s) Train Acc: 0.805994 Loss: 0.004186\n",
      "[020/030] 11.57 sec(s) Train Acc: 0.817326 Loss: 0.003958\n",
      "[021/030] 11.61 sec(s) Train Acc: 0.824354 Loss: 0.003787\n",
      "[022/030] 11.63 sec(s) Train Acc: 0.835222 Loss: 0.003589\n",
      "[023/030] 11.70 sec(s) Train Acc: 0.846337 Loss: 0.003356\n",
      "[024/030] 11.81 sec(s) Train Acc: 0.855564 Loss: 0.003146\n",
      "[025/030] 11.86 sec(s) Train Acc: 0.857979 Loss: 0.003074\n",
      "[026/030] 11.74 sec(s) Train Acc: 0.873862 Loss: 0.002795\n",
      "[027/030] 11.71 sec(s) Train Acc: 0.875286 Loss: 0.002711\n",
      "[028/030] 11.66 sec(s) Train Acc: 0.885751 Loss: 0.002539\n",
      "[029/030] 11.64 sec(s) Train Acc: 0.891789 Loss: 0.002401\n",
      "[030/030] 11.59 sec(s) Train Acc: 0.895226 Loss: 0.002290\n"
     ]
    }
   ],
   "source": [
    "model_best = Classifier().cuda()\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_best.parameters(), lr=0.001)\n",
    "num_epoch = 30\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc = 0.0\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model_best.train()\n",
    "    for i, data in enumerate(train_val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred = model_best(data[0].cuda())\n",
    "        batch_loss = loss(train_pred, data[1].cuda())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "        train_loss+= batch_loss.item()\n",
    "        \n",
    "        # print out the result\n",
    "    print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f' % \\\n",
    "      (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "      train_acc/train_val_set.__len__(), train_loss/train_val_set.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_best, 'model_best.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "test_model = torch.load('model_best.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = MicroExpreDataset(test_x, test_y, test_transform)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3589"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.eval()\n",
    "prediction = []\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        test_pred = test_model(data[0].cuda())\n",
    "        test_label = np.argmax(test_pred.cpu().data.numpy(), axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3589"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6224575090554472"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc = np.sum(prediction == test_y) / test_set.__len__()\n",
    "test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过torch.save保存整个模型，读取时会有一些报错问题。官方建议只保存训练参数，所以这边再存一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_best.state_dict(),'model_best_para.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
