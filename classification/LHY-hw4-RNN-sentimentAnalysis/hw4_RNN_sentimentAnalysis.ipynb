{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, data_mode):\n",
    "    with open(path, 'r') as f:\n",
    "        sentences = f.readlines()\n",
    "        if data_mode == \"train_with_label\":\n",
    "            x = [sentence.strip(\"\\n\").split(\" \")[2:] for sentence in sentences]\n",
    "            y = [sentence[0] for sentence in sentences]\n",
    "            return x, y\n",
    "        elif data_mode ==\"train_no_label\":\n",
    "            x = [sentence.strip(\"\\n\").split(\" \") for sentence in sentences]\n",
    "            return x\n",
    "        else:\n",
    "            x = [(\" \".join(sentence.strip(\"\\n\").split(\",\")[1:])).split(\" \") for sentence in sentences[1:]]\n",
    "            return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v.py\n",
    "# 训练word embedding的model\n",
    "def train_word2vec():\n",
    "    print(\"loading data...\")\n",
    "    train_x, _ = read_data(\"DATA/hw4/training_label.txt\", \"train_with_label\")\n",
    "    train_x_no_label = read_data(\"DATA/hw4/training_nolabel.txt\", \"train_no_label\")\n",
    "    test_x = read_data(\"DATA/hw4/testing_data.txt\",\"test\")\n",
    "    \n",
    "    print(\"train word2vec model...\")\n",
    "    # list累加--> concat\n",
    "    x = train_x + train_x_no_label + test_x\n",
    "    model = word2vec.Word2Vec(x, size=250, window=10, min_count=5, workers=12, iter=20, sg=1)\n",
    "    \n",
    "    print(\"saving model...\")\n",
    "    model.save(\"DATA/hw4/models/w2v_all.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "train word2vec model...\n",
      "saving model...\n"
     ]
    }
   ],
   "source": [
    "train_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess.py\n",
    "from gensim.models import Word2Vec\n",
    "class Preprocess():\n",
    "    def __init__(self, sentences, sen_len, w2v_path):\n",
    "        self.sentences = sentences\n",
    "        self.sen_len = sen_len\n",
    "        self.embedding = Word2Vec.load(w2v_path)\n",
    "        self.embedding_dim = self.embedding.vector_size\n",
    "        self.word2idx = {}\n",
    "        self.embedding_matrix = []\n",
    "    \n",
    "    # 补充embedding，例如PAD, UNK\n",
    "    def add_embedding(self, word):\n",
    "        vector = torch.empty(self.embedding_dim,1)\n",
    "        # 每次的值都不一样\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.word2idx[word] = len(self.word2idx)\n",
    "        self.embedding_matrix.append(vector)\n",
    "    \n",
    "    # 制作embedding matrix\n",
    "    def make_embedding(self):\n",
    "        for i, word in enumerate(self.embedding.wv.vocab):\n",
    "            self.word2idx[word] = len(self.word2idx)\n",
    "            self.embedding_matrix.append(self.embedding[word])\n",
    "        \n",
    "        # add <PAD> and <UNK> to embedding\n",
    "        self.add_embedding(\"<PAD>\")\n",
    "        self.add_embedding(\"<UNK>\")\n",
    "        \n",
    "        # 从二维list转为tensor\n",
    "        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n",
    "        return self.embedding_matrix\n",
    "    \n",
    "    # 把sentence截取或补全成一样长度\n",
    "    def pad_sentence(self, sentence):\n",
    "        if len(sentence) > self.sen_len:\n",
    "            sentence = sentence[:self.sen_len]\n",
    "        else:\n",
    "            pad_len = self.sen_len - len(sentence)\n",
    "            for _ in range(pad_len):\n",
    "                sentence.append(self.word2idx[\"<PAD>\"])\n",
    "        assert len(sentence) == self.sen_len\n",
    "        return sentence\n",
    "    \n",
    "    # 把训练集中每个sentence都转换成word embedding的形式\n",
    "    def sentence_word2idx(self):\n",
    "        # sentence_list中存的是每个sentence中word转为idx的list\n",
    "        # 其实也就是one hot的形式\n",
    "        sentence_list = []\n",
    "        for i, sen in enumerate(self.sentences):\n",
    "            sentence_idx = []\n",
    "            for word in sen:\n",
    "                if word not in self.word2idx.keys():\n",
    "                    word = \"<UNK>\"\n",
    "                sentence_idx.append(self.word2idx[word])\n",
    "            sentence_idx = self.pad_sentence(sentence_idx)\n",
    "            sentence_list.append(sentence_idx)\n",
    "        return torch.LongTensor(sentence_list)\n",
    "    \n",
    "    # 把 y 转成int\n",
    "    def labels_to_tensor(self, y):\n",
    "        y = [int(label) for label in y]\n",
    "        return torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.py\n",
    "# 需实现__init__(), __getitem__(), __len__()\n",
    "class TwitterDataset(data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = X\n",
    "        self.label = y\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is None: return self.data[idx]\n",
    "        else: return self.data[idx], self.label[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.py\n",
    "class LSTM_Net(nn.Module):\n",
    "    # 此处的embedding是embedding matrix\n",
    "    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n",
    "        super(LSTM_Net, self).__init__()\n",
    "        # embedding layer\n",
    "        self.embedding = torch.nn.Embedding(embedding.size(0), embedding.size(1))\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding)\n",
    "        \n",
    "        self.embedding.weight.requires_grad = False if fix_embedding else True\n",
    "        self.embedding_dim = embedding.size(1)\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.classifier = nn.Sequential(nn.Dropout(dropout),\n",
    "                                         nn.Linear(hidden_dim, 1),\n",
    "                                         nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        inputs = self.embedding(inputs)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        # x 的 dimension （batch, seq_len, hidden_size）\n",
    "        # 取 LSTM 最后一层的hidden state\n",
    "        x = x[:, -1, :]\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(outputs, labels):\n",
    "    # outputs 是 probability（float, 0.0 ~ 1.0）\n",
    "    # labels 是 float( 0 or 1)\n",
    "    outputs[outputs>=0.5] = 1\n",
    "    outputs[outputs<0.5] = 0\n",
    "    correct = torch.sum(torch.eq(outputs, labels)).item()\n",
    "\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n",
    "    # 全部的参数\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    # 需要模型训练的参数\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"\\n start training, parameter total:{}, trainable:{}\\n\".format(total, trainable))\n",
    "    \n",
    "    \n",
    "    # 定义损失函数, BCELoss --> Binary cross entropy loss\n",
    "    criteration = nn.BCELoss()\n",
    "    t_batch = len(train) # 此处train的len，即代表train中有几个batch\n",
    "    v_batch = len(valid) # 同上\n",
    "    # optimizer定义为Adam\n",
    "    optimizer = optim.Adam( model.parameters(), lr=lr)\n",
    "    # 记录模型训练\n",
    "    total_loss, total_acc, best_acc = 0, 0, 0\n",
    "    \n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        # 模型设为train模式\n",
    "        model.train()\n",
    "        total_loss, total_acc = 0, 0\n",
    "        # 做 training\n",
    "        for i, (inputs, labels) in enumerate(train):\n",
    "            # device 为 cuda， inputs转为 torch.cuda.LongTensor\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            # device 为 cuda，inputs转为 torch.cuda.FloatTensor, 因为要给criteration，所以需要是float\n",
    "            labels = labels.to(device, dtype=torch.float)\n",
    "            \n",
    "            # 保证每个batch训练前 loss.backward()的gradient归零，否则会自动累加\n",
    "            optimizer.zero_grad()\n",
    "            # 把 input 喂给模型\n",
    "            outputs = model(inputs)\n",
    "            # 把outputs最外层dimension去掉，以喂给criteration\n",
    "            outputs = outputs.squeeze()\n",
    "            # 把outputs给criteration，计算当前loss\n",
    "            loss = criteration(outputs, labels)\n",
    "            # 计算当前loss的gradient\n",
    "            loss.backward()\n",
    "            # 更新训练模型参数\n",
    "            optimizer.step()\n",
    "            # 计算此时模型的acc\n",
    "            correct = evaluation(outputs, labels)\n",
    "            \n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += (correct / batch_size)\n",
    "            \n",
    "            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f}% '.format(\n",
    "                epoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n",
    "        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n",
    "        \n",
    "        \n",
    "        # 做 validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss, total_acc = 0, 0\n",
    "            for i, (inputs, labels) in enumerate(valid):\n",
    "                inputs = inputs.to(device, dtype=torch.long)\n",
    "                labels = labels.to(device, dtype=torch.float)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.squeeze()\n",
    "                loss = criteration(outputs, labels)\n",
    "                correct = evaluation(outputs, labels)\n",
    "                \n",
    "                total_acc += (correct / batch_size)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n",
    "            \n",
    "            # 如果 total_acc 比 best_acc 要好，更新best_acc\n",
    "            if total_acc > best_acc:\n",
    "                best_acc = total_acc\n",
    "                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n",
    "                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n",
    "        print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TITAN RTX'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "loading data...\n",
      "preprocess data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouzy/softwares/Python-3.7.3/install/lib/python3.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build dataloader...\n",
      "building model...\n",
      "\n",
      " start training, parameter total:14447001, trainable:502251\n",
      "\n",
      "[ Epoch1: 1485/1485 ] loss:0.350 acc:30.469% \n",
      "Train | Loss:0.50145 Acc: 74.080\n",
      "Valid | Loss:0.42819 Acc: 79.638 \n",
      "saving model with acc 79.638\n",
      "-----------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouzy/softwares/Python-3.7.3/install/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type LSTM_Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Epoch2: 1485/1485 ] loss:0.440 acc:32.031% \n",
      "Train | Loss:0.41722 Acc: 80.865\n",
      "Valid | Loss:0.41070 Acc: 80.311 \n",
      "saving model with acc 80.311\n",
      "-----------------------------------------------\n",
      "[ Epoch3: 1485/1485 ] loss:0.370 acc:32.812% \n",
      "Train | Loss:0.39673 Acc: 82.109\n",
      "Valid | Loss:0.39789 Acc: 81.201 \n",
      "saving model with acc 81.201\n",
      "-----------------------------------------------\n",
      "[ Epoch4: 1485/1485 ] loss:0.350 acc:33.594% \n",
      "Train | Loss:0.37913 Acc: 82.943\n",
      "Valid | Loss:0.39271 Acc: 81.250 \n",
      "saving model with acc 81.250\n",
      "-----------------------------------------------\n",
      "[ Epoch5: 1485/1485 ] loss:0.406 acc:31.250% \n",
      "Train | Loss:0.36120 Acc: 83.960\n",
      "Valid | Loss:0.39216 Acc: 81.784 \n",
      "saving model with acc 81.784\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# 超参数\n",
    "fix_embedding = True\n",
    "batch_size = 128\n",
    "sen_len = 30 # 每个句子30个字\n",
    "epoch = 5\n",
    "lr = 0.001\n",
    "model_dir = \"DATA/hw4/models/\"\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "print(\"loading data...\")\n",
    "train_x, y = read_data(\"DATA/hw4/training_label.txt\", \"train_with_label\")\n",
    "train_x_no_label = read_data(\"DATA/hw4/training_nolabel.txt\", \"train_no_label\")\n",
    "test_x = read_data(\"DATA/hw4/testing_data.txt\",\"test\")\n",
    "\n",
    "# 对数据做预处理\n",
    "print(\"preprocess data...\")\n",
    "w2v_path = \"DATA/hw4/models/w2v_all.model\"\n",
    "preprocess = Preprocess(train_x, sen_len, w2v_path)\n",
    "# 得到embedding即embedding matrix\n",
    "embedding = preprocess.make_embedding()\n",
    "# 把train_x中每条数据（即每个句子）中每个词转换成idx的形式，每个句子相当于一条数据\n",
    "train_x = preprocess.sentence_word2idx()\n",
    "# 把 y 转换成 int 形式\n",
    "y = preprocess.labels_to_tensor(y)\n",
    "\n",
    "\n",
    "# 把data分为training data和validation data\n",
    "print(\"build dataloader...\")\n",
    "X_train, X_val, y_train, y_val = train_x[:190000], train_x[190000:], y[:190000], y[190000:]\n",
    "\n",
    "# 把data做成dataset供dataloader使用\n",
    "train_dataset = TwitterDataset(X = X_train, y = y_train)\n",
    "val_dataset = TwitterDataset(X = X_val, y = y_val)\n",
    "\n",
    "# 把dataset放到dataloader里面，转成batch of tensors\n",
    "train_loader = torch.utils.data.DataLoader( dataset = train_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = True,\n",
    "                                            num_workers = 8)\n",
    "val_loader = torch.utils.data.DataLoader(  dataset = val_dataset,\n",
    "                                            batch_size = batch_size,\n",
    "                                            shuffle = False,\n",
    "                                            num_workers = 8)\n",
    "\n",
    "# 制作一个model\n",
    "print(\"building model...\")\n",
    "model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=250, num_layers=1, dropout=0.5,\n",
    "                fix_embedding=fix_embedding)\n",
    "# device 如果是“cuda”，model就使用GPU来训练\n",
    "model = model.to(device)\n",
    "\n",
    "training( batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.py\n",
    "# 对testing_data.txt做预测\n",
    "def testing(test_loader, model, device):\n",
    "    # 在evaluation模式下进行预测\n",
    "    model.eval()\n",
    "    ret_output = []\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in enumerate(test_loader):\n",
    "            inputs = inputs.to(device, dtype=torch.long)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.squeeze()\n",
    "            outputs[outputs>=0.5] = 1\n",
    "            outputs[outputs<0.5] = 0\n",
    "            print(outputs)\n",
    "            print(outputs.int())\n",
    "            print(outputs.int().tolist())\n",
    "            print(a)\n",
    "            ret_output += outputs.int().tolist()\n",
    "    return ret_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhouzy/softwares/Python-3.7.3/install/lib/python3.7/site-packages/ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n",
      "tensor([0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 0.], device='cuda:0')\n",
      "tensor([0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
      "        1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0], device='cuda:0', dtype=torch.int32)\n",
      "[0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-b0196164e08d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ckpt.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 写到csv中\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-d547f0043876>\u001b[0m in \u001b[0;36mtesting\u001b[0;34m(test_loader, model, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m             \u001b[0mret_output\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "# predict and write to csv file\n",
    "print(\"loading testing data...\")\n",
    "test_x = read_data(\"DATA/hw4/testing_data.txt\",\"test\")\n",
    "\n",
    "# test时，也要记得做embedding处理\n",
    "preprocess = Preprocess(test_x, sen_len, w2v_path)\n",
    "embedding = preprocess.make_embedding()\n",
    "test_x = preprocess.sentence_word2idx()\n",
    "\n",
    "test_dataset = TwitterDataset(X=test_x, y=None)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers = 8)\n",
    "\n",
    "print(\"loading model...\")\n",
    "model = torch.load(os.path.join(model_dir, \"ckpt.model\"))\n",
    "outputs = testing(test_loader, model, device)\n",
    "\n",
    "# 写到csv中\n",
    "tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))], \"label\":outputs})\n",
    "print(\"saving csv...\")\n",
    "tmp.to_csv(os.path.join(model_dir, \"predict.csv\"), index=False)\n",
    "print(\"Finish predicting\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
